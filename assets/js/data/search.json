[
  
  {
    "title": "AudioKit으로 만드는 다중트랙 미디 재생기 (MultiTrack MIDI Player)",
    "url": "/posts/MultiTrackMIDIPlayer/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-23 17:16:31 +0900",
    "content": "Multitrack MIDI Player     코드 보기   이 코드는 AudioKit을 사용하여 여러 트랙과 악기를 포함한 MIDI 파일을 재생할 수 있는 SwiftUI 기반의 멀티트랙 MIDI 플레이어입니다. 구성은 크게 MultitrackMIDIPlayerConductor(오디오 및 MIDI 처리 로직)와 MultitrackMIDIPlayerView(UI)로 나뉘며, 아래와 같이 작동합니다.    MultitrackMIDIPlayerConductor: 오디오 및 MIDI 제어 클래스  주요 프로퍼티  let engine = AudioEngine() let sampler = MIDISampler() let sequencer = AppleSequencer() var samplers: [MIDISampler] = [] var mixer: Mixer!      AudioEngine: 오디오 출력의 핵심 객체   AppleSequencer: MIDI 트랙을 다루기 위한 시퀀서   samplers: 트랙별 악기(MIDISampler) 배열   mixer: 여러 sampler를 하나로 합쳐 출력     init()  sequencer.loadMIDIFile(\"MIDI Files/Horde3\") setTracks() setMixerOutput()   앱 시작 시 기본 MIDI 파일(Horde3)을 불러오고, 트랙 구성 및 믹서 설정을 실행합니다.    loadMIDIFile(url:)  sequencer.loadMIDIFile(fromURL: url) samplers.removeAll() setTracks() setMixerOutput()      .fileImporter를 통해 선택한 MIDI 파일을 로드   새로운 MIDI 트랙에 맞게 sampler를 재구성하고 믹서도 다시 설정   ※ engine.output = mixer가 setMixerOutput() 안에서 설정되므로 이 라인 이후 오디오 출력이 재설정됨    setTracks()  let tracks = sequencer.tracks ... let sampler = MIDISampler() try sampler.loadSoundFont(...) track.setMIDIOutput(sampler.midiIn)   각 트랙에 대해:     channel == 9인지 판단하여 드럼인지 구분   bank 값을 일반(0) 또는 드럼용(128)으로 설정   programChangeEvents에서 preset 번호 추출   해당 사운드폰트 로딩   트랙과 sampler 연결 → track.setMIDIOutput(...)   로그는 conductor.midiLog에 텍스트로 기록됨    sequencerPlay() / sequencerStop()  sequencer.play() sequencer.stop()   플레이/스톱 버튼에 대응하여 시퀀서를 재생 또는 정지합니다.    MultitrackMIDIPlayerView: 사용자 인터페이스  상태 바인딩  @StateObject private var conductor = MultitrackMIDIPlayerConductor() @State private var showFileImporter = false      conductor: 로직을 담당하는 ViewModel   showFileImporter: 파일 가져오기 창 제어용     UI 구성 요약     MIDI 파일명 표시   [Load MIDI File], [PLAY / STOP] 버튼   MIDI 분석 로그 스크롤 출력   Text(conductor.fileName) ... ScrollView {   Text(conductor.midiLog) }     파일 가져오기 (.midi)  .fileImporter(isPresented: $showFileImporter, allowedContentTypes: [.midi]) { result in   ...   conductor.loadMIDIFile(url: url) }   유저가 파일을 선택하면 MIDI 파일을 Conductor에 전달하여 새롭게 로드합니다. 보안 스코프 리소스 접근 권한도 자동으로 획득함 (startAccessingSecurityScopedResource())    오디오 엔진 관리  .onAppear { conductor.start() } .onDisappear { conductor.stop() }   뷰가 등장할 때 AudioKit 엔진 시작, 사라질 때 정지    ✅ 핵심 특징 요약                 기능       구현 방식                       트랙별 악기 설정       programChangeEvents 기반으로 MIDISampler 생성 및 연결                 퍼커션 채널 감지       channel == 9이면 bank: 128로 로딩                 MIDI 분석 출력       로그 텍스트를 ScrollView에 표시                 파일 가져오기       .fileImporter를 통해 .mid 파일 로딩                 실시간 재생 제어       sequencer.play() / sequencer.stop() 호출             📦 확장 아이디어     트랙별 볼륨/음소거 슬라이더 추가   ProgramChange 다중 처리 (중간 악기 변경)  "
  },
  
  {
    "title": "AudioKit: MIDISampler+AppleSequencer를 이용한 미디 멀티트랙 재생 중 트랙 중간에 프로그램 체인지가 있는 경우 대응 방법",
    "url": "/posts/%EB%AF%B8%EB%94%94_%ED%8A%B8%EB%9E%99%EC%A4%91%EA%B0%84_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8%EC%B2%B4%EC%9D%B8%EC%A7%80_%EB%8C%80%EC%9D%91%EB%B2%95/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-23 15:22:38 +0900",
    "content": "MIDISampler+AppleSequencer를 이용한 미디 멀티트랙 재생 중 트랙 중간에 프로그램 체인지가 있는 경우 대응 방법  track 2: [   AudioKit.MIDIProgramChangeEvent(time: 0.0, channel: 2, number: 57),   AudioKit.MIDIProgramChangeEvent(time: 64.0, channel: 2, number: 60),   AudioKit.MIDIProgramChangeEvent(time: 119.98697916666667, channel: 2, number: 57) ]   track 2처럼 트랙 중간에 Program Change 메시지로 악기가 바뀌는 경우, AudioKit의 AppleSequencer와 MIDISampler 조합만으로는 정확한 사운드 전환을 구현하기 어렵습니다.  기본적으로 track.setMIDIOutput(sampler.midiIn)으로 연결된 MIDISampler는 트랙 전체를 하나의 악기로만 재생합니다. 중간에 악기를 바꾸는 기능은 직접 처리해주지 않으면 반영되지 않습니다.    문제 상황 요약  track 2: [   ProgramChange(channel: 2, number: 57) // 시작 시   ProgramChange(channel: 2, number: 60) // 64초쯤   ProgramChange(channel: 2, number: 57) // 120초쯤 복귀 ]      표준 MIDI에서는 이런 식으로 Program Change 메시지를 트랙 내에 삽입하여 동일 채널에 악기 전환을 수행   AudioKit의 기본 Sequencer 처리 방식에서는 이를 자동 인식하거나 대응하지 않음     🎯 해결 방향  ① Program Change 이벤트를 파싱해서     시점별로 악기(프리셋) 변경 정보를 추출   ② 해당 채널의 MIDI 이벤트를 악기 구간별로 나눠서     구간별 MIDISampler를 생성하거나   하나의 Sampler에 프리셋을 실시간으로 변경 요청 (단, 대부분의 MIDISampler는 실행 중 preset 변경을 안정적으로 지원하지 않음)     실현 가능한 접근 방법  방법 A: 트랙을 구간별로 분할하여 각각 sampler에 할당     1개 트랙을 Program Change 기준으로 여러 개로 나누고, 구간마다 MIDISampler와 연결 → Mixer에 병합   단점:     구현 복잡도 있음   MIDI 이벤트 재구성이 필요   방법 B: ProgramChange 메시지 수신 시 sampler의 preset을 변경  if case let .programChange(channel, number) = event {   if channel == 2 {     try? sampler.loadSoundFont(\"CT8MGM\", preset: number, bank: 0)   } }      MIDI 이벤트를 수신하는 커스텀 MIDIListener 구현 필요   실행 중 preset을 바꾸면 버퍼 끊김이나 글리치 발생 가능      실행 중 preset 변경은 대부분 권장되지 않음     권장 방식 (현실적 대안)  🎯 트랙 중간에 Program Change가 있는 경우:     MIDI 파일을 외부에서 미리 분할하거나   DAW에서 동일 채널에 Program Change를 쓰지 않도록 재구성   또는 AudioKit으로 다룰 경우, 중간 ProgramChange를 무시하고 시작 preset만 사용     요약                 상황       설명                       트랙에 ProgramChange가 여러 개 있음       MIDI 채널의 악기가 재생 중에 바뀜                 AudioKit AppleSequencer       한 트랙당 하나의 sampler만 연결됨                 실시간 preset 변경       가능은 하지만 glitch 발생 가능                 현실적인 방법       ProgramChange 기준으로 트랙을 사전 분할 또는 첫 preset만 사용           "
  },
  
  {
    "title": "AudioKit의 MID Port Test",
    "url": "/posts/MIDIPortTest/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-23 00:42:28 +0900",
    "content": "MIDI Port Test (Virtual MIDI Host 포함)     코드 보기   MIDIPortTestConductor는 AudioKit과 MIDIKit을 이용해 MIDI 포트를 테스트하거나 MIDI 이벤트를 수신/전송할 수 있도록 설계된 MIDI 테스트 도구의 핵심 로직 클래스입니다. 가상 포트를 생성하고, 외부 장치와 통신하며, MIDI 로그를 저장하고, 포트 정보를 조회하는 기능을 포함합니다.    주요 목적     가상 MIDI 입/출력 포트 생성 및 관리   외부 MIDI 장치 연결 및 포트 열기/닫기   MIDI 이벤트 실시간 수신 및 로그 기록   특정 포트로 이벤트 전송   테스트용 UI와의 연결을 위한 ObservableObject 구현     주요 구성요소 설명  고정 UID 정의  let inputUIDDevelop: Int32 = 1_200_000 let outputUIDDevelop: Int32 = 1_500_000 let inputUIDMain: Int32 = 2_200_000 let outputUIDMain: Int32 = 2_500_000      고정된 UID를 가진 가상 포트를 생성하기 위해 사용됨   UID 기반으로 포트 교체(swap)하거나 구분 가능     @Published 속성  @Published var log = [MIDIEvent]() @Published var outputIsOpen: Bool @Published var outputPortIsSwapped: Bool @Published var inputPortIsSwapped: Bool      UI에서 실시간으로 바뀌는 값을 추적할 수 있도록 상태 노출   MIDI 이벤트 기록(log), 포트 스왑 여부, 출력 포트 오픈 여부 관리     MIDI 초기화  midi.destroyAllVirtualPorts() midi.createVirtualInputPorts(...) midi.createVirtualOutputPorts(...) midi.addListener(self)      기존 포트를 제거하고, UID를 지정한 가상 포트 생성   self를 MIDIListener로 등록하여 수신 처리 가능     로그 버퍼링 및 Timer 처리  private var logBuffer = [MIDIEvent]() private var logTimer: Timer?      MIDI 이벤트를 실시간으로 logBuffer에 임시 저장   0.5초마다 한 번씩 flushLogBuffer()를 호출해 log에 삽입 → UI 성능 저하 방지     포트 설명 캐싱  private var portDescriptionCache: [MIDIUniqueID : PortDescription]      inputPortDescription(forUID:) 함수에서 매번 조회하지 않고 캐싱   초기화 시 백그라운드 스레드에서 inputInfos를 순회해 캐싱해둠     포트 열기 및 닫기  func didSetOutputIsOpen() { ... } func openOutputs() { ... } func start() / stop()      outputIsOpen이 바뀔 때 포트를 열거나 닫음   start()는 input 포트 열기, stop()은 모두 닫기     이벤트 전송  func sendEvent(eventToSend:event, portIDs:[MIDIUniqueID]?)           MIDIEvent 객체를 보고 적절한 AudioKit 전송 함수 호출             .controllerChange → sendControllerMessage       .programChange → sendEvent(...)       .noteOn / .noteOff → 각각 전송                swapVirtualOutputPorts()로 포트 교체 가능        포트 스왑 기능  func swapVirtualInputPort(...) func swapVirtualOutputPorts(...)      개발용 포트와 메인 포트를 교체하여 라우팅 방식 변경 가능   예: 출력 포트 UID가 outputUIDDevelop이면 → inputUIDDevelop로 바꿈     MIDIListener 구현  receivedMIDINoteOn(...) receivedMIDIController(...) ...      MIDIKit이 제공하는 이벤트 콜백   수신된 이벤트를 logBuffer에 추가   로그에 포트 ID 포함 → 어떤 포트에서 들어온 메시지인지 추적 가능     요약                 기능       설명                       가상 포트 생성       고정 UID로 input/output 포트 생성                 포트 상태 토글       outputIsOpen으로 열고 닫음                 포트 스왑       개발용/메인 포트를 상황에 따라 교체                 MIDI 이벤트 수신       MIDIListener 프로토콜 구현                 이벤트 로그 기록       logBuffer + 타이머 → @Published log                 포트 정보 조회       UID → 제조사/장치 이름 조회 및 캐싱                 이벤트 전송       noteOn/noteOff/controller/program 등 구분 전송             MIDIPortTestView  MIDIPortTestView는 AudioKit + MIDIKit 기반의 SwiftUI 뷰로, 가상 및 외부 MIDI 포트의 입출력 테스트, CC/노트 전송, 포트 선택 및 실시간 로그 확인 기능을 제공합니다. 앱 또는 MIDI 툴에서 MIDI 포트 동작 상태를 시각적으로 테스트하거나 디버그하는 데 유용한 구성입니다.    전체 구조 요약  MIDIPortTestView  ├─ HeaderArea                     ← 포트 개수 및 이름 헤더 표시  ├─ TabView                       ← 포트1/포트2 전환 UI  │   ├─ Port1SelectArea           ← Destination 포트 선택  │   ├─ Port2SelectArea           ← Virtual Output 포트 선택  │   └─ PortEventArea(portID)     ← 선택 포트로 Note/CC 전송 버튼들  ├─ 포트 스왑/출력 여부 Toggle  ├─ LogResetButtonArea            ← 로그 초기화 버튼  ├─ LogHeaderArea                 ← MIDI 로그의 컬럼 헤더  └─ LogDataArea                   ← 수신된 MIDI 로그 목록 표시     주요 구성 요소 설명  HeaderArea  HeaderCell(...) // 4칸      현재 MIDI 시스템에 등록된 포트 정보(입력/출력/가상 등)의 개수, 이름, UID를 표시     TabView + Port1/2SelectArea  TabView { VStack { Port1SelectArea ... } }      PageTabViewStyle로 구성   첫 페이지: 외부 Destination 포트 선택 + 버튼 전송   두 번째 페이지: Virtual Output 포트 선택 + 전송     PortEventArea(portID:)  MIDIEventButton(eventToSend: .noteOn / .noteOff ...)      하드코딩된 노트 번호([60, 62, 64, 67, 69] 등)로 NoteOn/NoteOff 전송 버튼   ProgramChange(랜덤 악기 변경), CC 1 (Mod Wheel) 전송 버튼 포함   버튼 클릭 시 conductor.sendEvent(...) 호출로 해당 포트에 MIDI 전송     로그 관련 UI  LogHeaderArea     상태, 채널, 데이터1~2, 포트 UID, 장치명, 제조사명 등 컬럼 헤더 출력   LogDataArea  ForEach(conductor.log.indices) { i in ... }      실시간 MIDI 이벤트 로그 출력   한 줄당 .LazyHStack으로 7개 항목 출력   conductor.inputPortDescription(forUID:)를 통해 포트 UID → 이름/제조사 변환 (성능 저하 원인)     포트 상태 전환 토글  Toggle(isOn: $conductor.outputIsOpen) { ... } Toggle(isOn: $conductor.inputPortIsSwapped) { ... } Toggle(isOn: $conductor.outputPortIsSwapped) { ... }      outputIsOpen: openOutputs() 실행 여부   inputPortIsSwapped: 가상 입력 포트 UID 스왑 여부   outputPortIsSwapped: 가상 출력 포트 UID 스왑 여부 → conductor.swapVirtualInputPort(...) 등의 포트 경로 제어에 영향     기능 요약                 기능       설명                       포트 선택       외부 포트 및 가상 포트 드롭다운 선택                 MIDI 이벤트 전송       NoteOn/Off, CC, ProgramChange 전송                 포트 스왑       가상 포트 UID를 상황에 따라 교체                 로그 보기       수신된 MIDI 이벤트 로그 실시간 표시                 로그 초기화       버튼으로 로그 지우기             주의/개선 포인트     LogDataArea에서 inputPortDescription()은 성능 병목 → 캐싱 또는 비동기 평가 필요   포트 수가 많을 경우 Picker나 log 처리 성능 저하 발생 가능   각 포트별 상태나 활성화 여부도 추가 UI로 제공 가능     요약  MIDIPortTestView는:     가상/외부 MIDI 포트를 선택하고   테스트용 MIDI 이벤트를 보내며   수신되는 MIDI 로그를 실시간으로 시각화하는 데 최적화된 SwiftUI 뷰입니다.   테스트 도구, 디버깅 뷰, 또는 DAW 플러그인 개발자용 MIDI 유틸리티로 활용할 수 있습니다.   "
  },
  
  {
    "title": "AudioKit의 PolyphonicSTK+MIDIKit",
    "url": "/posts/PolyphonicSTK+MIDIKit/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-22 19:03:37 +0900",
    "content": "Polyphonic STK + MIDIKit     코드 보기   이 코드는 AudioKit과 MIDIKit을 함께 사용하여, 가상 또는 외부 MIDI 장치로부터 들어오는 MIDI 이벤트를 수신하고, RhodesPianoKey(SoundpipeAudioKit) 기반으로 폴리포닉 연주를 가능하게 하는 구조입니다.    MIDIKit 관련 설명  MIDIManager 생성  let midiManager = MIDIManager(   clientName: \"CookbookAppMIDIManager\",   model: \"CookbookApp\",   manufacturer: \"BGSMM\" )      MIDIKit의 핵심 객체   CoreMIDI 클라이언트를 만들고, 포트와 연결을 관리함   iOS/macOS의 MIDI 시스템과 연동됨     MIDI 연결 설정: MIDIConnect()  1. MIDI 서비스 시작  try midiManager.start()      MIDI 시스템 접근을 시작하며, 장치 탐색 및 연결 가능 상태로 전환   2. 입력 연결 설정  try midiManager.addInputConnection(   to: .allOutputs,   tag: \"Listener\",   filter: .owned(),   receiver: .events { ... } )      .allOutputs: 연결 가능한 모든 외부 출력 포트를 수신 대상으로 설정   filter: .owned(): 본 앱이 만든 가상 포트는 수신 대상에서 제외   receiver: .events: 이벤트 수신 핸들러     이벤트 수신 핸들러  receiver: .events { [weak self] events, timeStamp, source in   Task { @MainActor in     for event in events {       self?.received(midiEvent: event)     }   } }      비동기 클로저이며 백그라운드 스레드에서 호출됨   Task { @MainActor in ... }으로 UI 안전하게 업데이트   self는 weak으로 캡처하여 메모리 누수 방지      @Sendable 제한을 피하기 위해 DispatchQueue.main.async 대신 Task { @MainActor } 사용     수신된 MIDI 이벤트 처리  private func received(midiEvent: MIDIEvent) { ... }      MIDI 이벤트 타입별로 분기 처리                  타입       처리 내용                       .noteOn       음을 재생 + NotificationCenter로 노트 정보 전달                 .noteOff       해당 음을 멈춤 + NotificationCenter로 노트 정보 전달                 .cc       콘트롤 체인지 정보 콘솔 출력                 .programChange       프로그램 체인지 이벤트 출력                 기타       무시              NotificationCenter.default.post(...)는 외부 UI에 키보드 상태 전파에 사용됨     AudioKit 관련 (간단 요약)     RhodesPianoKey → 기본 오실레이터   AmplitudeEnvelope으로 각각 음의 게이트 제어   최대 11음까지 동시에 연주 가능   .noteOn, .noteOff로 envs[i].openGate() / closeGate() 처리   출력은 Mixer(envs) → engine.output     🧪 동작 요약     앱 실행 → MIDIManager 시작   가상포트 혹은 실제 MIDI 키보드에서 노트를 누름   .noteOn 수신 → noteOn(pitch:) 호출   RhodesPianoKey 연주 시작   .noteOff 수신 → noteOff(pitch:) → 게이트 닫힘     사용 예     외부 MIDI 장치, 가상 MIDI 장치, DAW 등에서 이 앱으로 노트 전송 가능   SwiftUI UI에서 MIDIKItKeyboard로 직접 연주도 가능   모든 이벤트는 콘솔과 UI에 실시간 반영됨    "
  },
  
  {
    "title": "AudioKit의 Roland TB303 Filter",
    "url": "/posts/Roland_TB303_Filter/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-22 17:36:28 +0900",
    "content": "Roland TB-303 Filter  RolandTB303Filter는 Roland의 전설적인 베이스 신시사이저 TB-303의 필터 사운드를 모방한 레조넌트 로우패스 필터입니다. TB-303은 1980년대에 등장한 아날로그 신스 베이스 머신으로, 애시드 하우스(acid house)와 전자 음악 장르에서 매우 유명한 독특한 필터 사운드를 제공합니다.    🎛️ Roland TB-303 Filter란?     1-pole 혹은 4-pole 저역 통과 필터(Low-Pass Filter)로 고주파수를 잘라내고 저역만 통과시킵니다.   자극적이고 공격적인 레조넌스 특성으로 유명하며, 레조넌스를 극단적으로 높일 때 오실레이터처럼 자가 발진(self-oscillate) 하는 특성도 있음.   SoundpipeAudioKit에 구현된 RolandTB303Filter는 이 클래식한 아날로그 필터의 사운드 특성과 비선형 동작(디스토션, 비대칭성)을 디지털로 복제한 것입니다.     ⚙️ 파라미터 목록 및 효과 설명  1. Cutoff Frequency  | 500.0 | 12.0...20000.0 Hz     역할: 필터가 신호를 감쇄하기 시작하는 주파수   낮은 값: 저음만 통과 (우우우 하는 어두운 톤)   높은 값: 고음도 통과 → 밝고 날카로운 사운드      동적 제어 시 “와우 와우” 같은 자동 필터 스윕 효과 가능     2. Resonance  | 0.5 | 0.0...2.0     역할: 컷오프 주파수 근처의 특정 주파수를 강조(공명)   낮은 값 (0.0~0.3): 부드럽고 무난한 필터   중간 값 (0.5~1.0): TB-303 특유의 콱콱 찌르는 듯한 어택 생김   높은 값 (&gt;1.5): 거의 자체 발진에 가까운 강한 휘파람/휘웅 소리      TB-303 스타일 애시드 사운드의 핵심     3. Distortion  | 2.0 | 0.0...4.0     역할: 필터 후단에 적용되는 비선형 오버드라이브 효과   낮은 값: 깨끗하고 자연스러운 필터   중간 값 (~2.0): 살짝 과장된 빈티지 톤   높은 값 (4.0): 거칠고 찢어지는 음색 → 애시드 사운드 강화      디지털 필터의 지나치게 깔끔한 소리를 거칠게 만들기 위한 파라미터     4. Resonance Asymmetry  | 0.5 | 0.0...1.0     역할: 공명 피크(레조넌스)의 파형 비대칭도 제어   0.0: 완전히 대칭적인 필터 반응   1.0: 비대칭 공명 → 비자연적이고 디스토션된 느낌   0.5: TB-303 특유의 약간 찌그러진 공명 표현 가능      미세 조정용 파라미터로, 전체적인 톤의 “캐릭터”를 정함     🧪 파라미터 조합 예시                 Cutoff       Resonance       Distortion       Asymmetry       결과 사운드                       800       0.8       1.5       0.5       기본적인 TB-303 베이스                 200       1.8       3.0       1.0       극단적 애시드 사운드, 거칠고 공격적                 1000       0.2       0.0       0.0       부드럽고 깨끗한 베이스 필터링                 1500       1.0       4.0       0.3       고역 강조, 디스토션된 리드 사운드             ✅ 요약     RolandTB303Filter는 TB-303 베이스 신스의 핵심 요소인 레조넌트 로우패스 필터를 재현한 것   Cutoff + Resonance의 조합이 사운드 캐릭터의 중심   Distortion은 날카로운 빈티지 톤을 더함   Resonance Asymmetry는 필터의 비선형성을 미세하게 조절함  "
  },
  
  {
    "title": "AudioKit의 DunneSynth",
    "url": "/posts/DunneSynth/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-22 15:58:34 +0900",
    "content": "Dunne Synth     코드 보기   DunneSynthConductor는 AudioKit의 Synth (정확히는 DunneAudioKit의 Dunne Synthesizer)를 제어하는 오디오 신스 컨덕터 클래스입니다. SwiftUI 기반의 UI와 연동하여 신스의 파라미터 조정, 노트 재생, 피크 리미팅, 팝 제거 등 다양한 오디오 처리 제어를 담당합니다.    클래스 개요  class DunneSynthConductor: ObservableObject, HasAudioEngine      ObservableObject이므로 SwiftUI 뷰와 상태 바인딩 가능   HasAudioEngine 프로토콜을 통해 engine 및 노드 연결 보장   내부에서 Synth 객체를 사용하여 가상 악기를 구현     주요 구성 요소  let engine = AudioEngine()     AudioKit의 핵심 오디오 처리 엔진   var instrument = Synth()     DunneAudioKit의 폴리포닉 신스   매핑된 파라미터들을 실시간으로 제어 가능     noteOn / noteOff  func noteOn(pitch: Pitch, point _: CGPoint)  instrument.play(noteNumber: ..., velocity: 100, channel: 0)      전달된 Pitch를 MIDI note로 변환하여 재생 시작   func noteOff(pitch: Pitch)  instrument.stop(noteNumber: ..., channel: 0)      해당 음 높이의 노트를 정지시킴     초기화: init()  1. engine.output = PeakLimiter(...)     출력단에 PeakLimiter 삽입 → 소리의 과도한 피크 방지   attackTime = 0.001: 피크 감지 후 빠르게 제한   decayTime = 0.001: 제한 해제도 즉시   preGain = 0: 입력 게인을 증폭하지 않음   2. 팝 노이즈 제거 설정  instrument.releaseDuration = 0.01 instrument.filterReleaseDuration = 10.0 instrument.filterStrength = 40.0      releaseDuration이 너무 짧으면 소리가 갑자기 끊기면서 팝(Pop) 노이즈 발생   filterReleaseDuration을 길게 설정해 필터 계열 소리가 부드럽게 사라지도록 함   filterStrength는 필터 적용 정도 조절     파라미터 목록  일반 음성 파라미터                 파라미터       기본값       범위       설명                       Master Volume       1.0       0.0...1.0       전체 출력 볼륨                 Pitch Bend       0.0       -24.0...24.0       실시간 피치 변조 (세미톤 단위)                 Vibrato Depth       0.0       0.0...12.0       비브라토 강도                 Filter Cutoff       1.0       0.0...1.0       필터 컷오프 주파수 (정규화)                 Filter Strength       40.0       0.0...100.0       필터 효과의 강도                 Filter Resonance       0.0       -20.0...20.0       공명(Resonance) 강도             앰플리튜드 ADSR (Amplitude Envelope)                 파라미터       기본값       범위       설명                       Attack Duration       0.0       0.0...10.0       노트 시작 시 볼륨이 증가하는 시간                 Decay Duration       0.0       0.0...10.0       최대 볼륨에서 지속 볼륨까지 줄어드는 시간                 Sustain Level       1.0       0.0...1.0       키를 누르고 있을 때 유지되는 볼륨                 Release Duration       0.01       0.0...10.0       노트 종료 후 소리가 사라지는 시간             필터 ADSR (Filter Envelope)                 파라미터       기본값       범위       설명                       Filter Attack Duration       0.0       0.0...10.0       필터가 작동을 시작하는 데 걸리는 시간                 Filter Decay Duration       0.0       0.0...10.0       필터가 최대 효과에서 사라지는 시간                 Filter Sustain Level       1.0       0.0...1.0       필터 효과의 유지 비율                 Filter Release Duration       10.0       0.0...10.0       필터 효과가 사라지는 시간             콘솔 출력  instrument.parameters.forEach {   print(\"\\($0.def.name) | \\($0.value) | \\($0.range)\") }      모든 파라미터 이름, 현재값, 범위를 디버깅용으로 출력   UI 연결 확인이나 슬라이더 구성 시 유용함     요약  DunneSynthConductor는:     AudioKit의 Dunne 신스를 제어하고   소리의 피크와 팝 노이즈를 제어하며   다양한 신스 파라미터들을 외부 UI와 연결할 수 있게 준비된 컨트롤러입니다.      SwiftUI와 함께 사용할 때 매우 직관적인 신디사이저 앱이나 교육용 악기 UI를 구성하는 데 적합합니다.  "
  },
  
  {
    "title": "AudioKit의 InputDeviceDemo+ChannelRouting",
    "url": "/posts/InputDeviceDemo+ChannelRouting/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-22 15:36:36 +0900",
    "content": "Input Device Demo     코드 보기   이 SwiftUI + AudioKit 코드의 목적은 입력 장치(마이크)를 선택하고 오디오 입력을 시작/정지하는 UI를 제공하는 것입니다. 즉, 여러 마이크 장치가 연결된 환경에서 사용자가 원하는 입력을 선택하고, 마이크 버튼을 눌러 오디오 입력을 켜거나 끌 수 있게 합니다.     Channel/Device Routing 예제와 거의 동일     🔧 클래스: InputDeviceDemoConductor  ✅ 역할: 오디오 입력 장치 관리 + AudioKit 엔진 제어  주요 속성                 변수       설명                       engine       AudioKit의 AudioEngine 인스턴스                 mic       현재 연결된 오디오 입력 노드 (engine.input)                 mixer       입력을 오디오 엔진의 출력으로 연결하기 위한 믹서 노드                 inputDevices       AVAudioSession에서 얻은 사용 가능한 오디오 입력 장치 목록                 inputDeviceList       Picker에 표시할 장치 이름 목록 (portName 배열)           init()     오디오 입력 노드가 있을 경우 mic에 할당하고 Mixer(input)으로 연결   입력 장치 목록(inputDevices)을 불러와 portName을 inputDeviceList에 저장   engine.output = mixer를 통해 믹서를 엔진의 출력으로 설정   switchInput(number:)     선택된 입력 장치를 setPreferredInput(...)으로 변경   변경 전에는 stop() 호출로 엔진을 멈춤     🖼️ View: InputDeviceDemoView  ✅ 역할: UI를 통해 입력 장치 선택 및 오디오 입력 시작/정지  주요 상태 변수                 변수       설명                       isPlaying       오디오 입력이 현재 켜져 있는지 여부                 inputDevice       현재 선택된 입력 장치의 인덱스 (Picker에서 선택됨)             ✅ 뷰 구조 설명  📍 안내 텍스트  Text(\"Please plug in headphones\") Text(\"to avoid a feedback loop.\") Text(\"Then, select a device to start!\")      피드백(하울링)을 피하기 위해 이어폰 사용을 권장     🎛 입력 장치 선택 Picker  Picker(\"Input Device\", selection: $inputDevice) {   ForEach(conductor.inputDeviceList.indices, id: \\.self) { i in     Text(conductor.inputDeviceList[i])       .tag(i)   } }      conductor.inputDeviceList에 저장된 장치 이름을 기반으로 Picker 구성   선택이 변경되면 .onChange(of: inputDevice)를 통해 switchInput(...) 호출     🎤 마이크 on/off 버튼  Button {   isPlaying ? conductor.stop() : conductor.start()   isPlaying.toggle() }      클릭 시 마이크 입력이 시작되거나 종료   버튼의 아이콘은 isPlaying 상태에 따라 \"mic.circle\" 또는 \"mic.circle.fill\"로 바뀜   .resizable() + .frame(...)으로 크기와 정렬 지정     📤 종료 시 동작  .onDisappear {   conductor.stop() }      뷰가 사라질 때 자동으로 오디오 입력 종료     ✅ 전체 동작 요약                 상황       동작                       앱 시작       AudioKit 엔진 초기화 + 사용 가능한 입력 장치 목록 확보                 장치 선택       선택된 장치로 입력 전환 (switchInput)                 버튼 클릭       오디오 입력 시작/정지 (start() / stop())                 뷰 종료       입력 강제 종료             ✅ 확장 가능성     입력 장치별 채널 수 확인   입력을 파일로 녹음   출력 장치 설정도 추가 가능     이 코드는 AudioKit을 활용한 기초 오디오 입출력 제어 구조를 익히기에 적절한 예제입니다. "
  },
  
  {
    "title": "AudioKit에서 BaseTapForSpeechRecognition을 사용한 음성인식 예제",
    "url": "/posts/BaseTapForSpeechRecognition/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-21 22:06:46 +0900",
    "content": "Base Tap for Speech Recognition     코드 보기   현재 코드에서의 SpeechRecognitionTap과 BaseTapForSpeechRecognitionConductor는 AudioKit과 Speech Framework를 연결하여 실시간으로 마이크 입력을 받아 음성 인식 결과를 텍스트로 출력하는 역할을 수행합니다. 변경된 코드 기준으로 두 클래스의 구성과 동작을 자세히 설명드리겠습니다.    ✅ SpeechRecognitionTap: 마이크 오디오 버퍼를 음성 인식기로 전달  class SpeechRecognitionTap: BaseTap   BaseTap은 AudioKit에서 입력을 분석하기 위한 탭(Tap)을 구현할 때 사용하는 기본 클래스입니다. SpeechRecognitionTap은 이를 상속받아 Apple의 Speech 프레임워크와 연결하는 데 초점을 둡니다.  🔧 주요 프로퍼티                 프로퍼티       설명                       recognitionRequest       오디오 버퍼를 지속적으로 전달받는 객체 (SFSpeechAudioBufferRecognitionRequest)                 analyzer       언어별 음성 인식기 (SFSpeechRecognizer)                 reconitionTask       실제 인식 작업을 수행하는 비동기 작업 (SFSpeechRecognitionTask)           🔨 메서드 설명  setupRecognition(locale:)  func setupRecognition(locale: Locale)      선택한 언어(Locale)에 맞는 SFSpeechRecognizer 인스턴스를 생성   실시간 스트리밍 인식을 위한 SFSpeechAudioBufferRecognitionRequest 초기화   recognitionRequest.shouldReportPartialResults = true 설정으로 중간 결과도 실시간 전달 가능   stopRecognition()  func stopRecognition()      음성 인식 세션을 종료하고 관련 객체를 초기화하여 해제   doHandleTapBlock(buffer:at:)  override func doHandleTapBlock(buffer: AVAudioPCMBuffer, at time: AVAudioTime)      AudioKit에서 자동으로 호출되는 메서드   마이크 입력 버퍼를 recognitionRequest에 전달하여 음성 인식기가 실시간으로 처리하게 함     ✅ BaseTapForSpeechRecognitionConductor: 음성 인식과 오디오 경로를 통합 관리  class BaseTapForSpeechRecognitionConductor: ObservableObject, HasAudioEngine   🔧 주요 프로퍼티                 프로퍼티       설명                       textString       인식된 텍스트 결과                 languageCode       선택된 언어 코드 (en-US, ko-KR, ja-JP)                 srTap       SpeechRecognitionTap 인스턴스                 engine, mic, outputMixer, silencer       AudioKit의 오디오 라우팅 구성 요소                 recognitionTask       음성 인식 작업 참조용           🔨 주요 로직  init()     오디오 엔진과 마이크 입력 초기화   AudioKit의 output = silencer로 출력 경로 설정 (실제 소리는 꺼둔 상태)   srTap.start()로 마이크 데이터 수집 시작   setLanguage(code:)를 호출하여 기본 언어로 음성 인식 시작   setLanguage(code:)  func setLanguage(code: String)      현재 음성 인식 세션을 중단 (stopRecognition, stop)   오디오 엔진을 재시작   새로운 언어로 SFSpeechRecognizer를 초기화하고 recognitionRequest를 설정   recognitionTask를 새로 만들어 textString에 실시간 결과를 반영      핵심: 언어가 바뀔 때 engine, tap, recognizer를 모두 재시작해서 새로운 인식 환경을 구성     🧠 흐름 요약     뷰가 나타남 → init()에서 오디오 및 인식기 설정   사용자가 언어를 선택 → languageCode.didSet → setLanguage(...)   마이크 입력 → doHandleTapBlock → recognitionRequest.append(...)   Apple 음성 인식기에서 처리된 결과가 textString에 실시간 업데이트됨   뷰는 ScrollView를 통해 자동 스크롤하며 결과를 출력     📌 정리                 클래스       역할       핵심 기능                       SpeechRecognitionTap       오디오 → SpeechFramework 전달       마이크 입력 버퍼를 recognitionRequest에 지속 전달                 BaseTapForSpeechRecognitionConductor       오디오 엔진 + 언어 변경 + 인식기 관리       언어 변경 시 자동 재설정, 실시간 결과 처리            "
  },
  
  {
    "title": "Tables",
    "url": "/posts/Tables/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-21 20:11:40 +0900",
    "content": "Tables     코드 보기   이 코드는 AudioKit의 Table 객체들을 생성하고 시각화하는 SwiftUI 뷰입니다. 특히 생성자(init())에서는 다양한 방식으로 **파형 테이블(Table)**을 생성하여 오실레이터 등에서 사용할 수 있도록 준비합니다. 이 Table들은 사인파, 정사각파, 사용자 정의 파형 등의 루프 가능한 단일 사이클 웨이브폼입니다.    ✅ 클래스 개요: TableConductor  이 클래스는 ObservableObject로, UI에서 이 객체의 @Published 값을 관찰할 수 있도록 구성되어 있습니다.  class TableConductor: ObservableObject   📌 속성 목록                 속성 이름       역할                       square       정사각형 파형 테이블 (Square Wave)                 triangle       삼각형 파형 테이블 (Triangle Wave)                 sine       사인파 테이블 (Sine Wave)                 sineHarmonic       하모닉(배음) 기반의 사인파                 fileTable       오디오 파일 기반 파형                 custom       사용자 정의 파형 (난수 + 인덱스 기반)             🧠 생성자 설명: init()  🔹 1. 기본 파형 테이블 생성  square = .init(.square, count: 128) triangle = .init(.triangle, count: 128) sine = .init(.sine, count: 256)      .init(_:count:)는 해당 파형 유형의 테이블을 지정된 크기만큼 생성   count는 테이블 길이 (샘플 수), 2의 제곱수로 설정하는 것이 일반적 (FFT 등 성능에 영향)     🔹 2. 오디오 파일 기반 파형 생성  let url = GlobalSource.piano.url! let file = try! AVAudioFile(forReading: url) fileTable = .init(file: file)!      .sfz 또는 .wav 등의 오디오 파일을 읽어 들여 파형 테이블로 변환   fileTable은 실제 음성 데이터를 기반으로 한 루프 테이블이 됨   주의:    파일 로딩은 오래 걸릴 수 있으므로 생성자에서 직접 하지 말고 task 등을 통해 비동기적으로 실행     🔹 3. 하모닉 오버톤 기반 테이블 생성  let harmonicOvertoneAmplitudes: [Float] = [   0.0, 0.0, 0.016, 0.301 ] sineHarmonic = .init(.harmonic(harmonicOvertoneAmplitudes), phase: 0.75)      AudioKit.TableType.harmonic(_)은 **기본파 + 배음(amplitudes)**으로 구성된 파형 생성   harmonicOvertoneAmplitudes[n]은 (n+1)번째 고조파의 세기   phase: 0.75는 위상 오프셋으로, 시작 지점을 오른쪽으로 75%만큼 이동   예: 배음 = [0.0, 0.0, 0.016, 0.301] → 기본파 없음, 3번째와 4번째 고조파만 있는 파형    🔹 4. 사용자 정의 파형 생성  custom = Table(.sine, count: 256) for i in custom.indices {   custom[i] += Float.random(in: -0.5...0.5) + Float(i) / 2048.0 }      먼저 사인파 테이블을 생성한 뒤, 각 샘플에 무작위 값 + 인덱스 기반 값을 더함   이는 잡음 성분이 섞인 사인파 또는 변형된 파형을 만들기 위한 목적   예:     Float.random(in: -0.5...0.5) → 랜덤 노이즈   Float(i) / 2048.0 → 위치에 따라 증가하는 값     🖼️ 테이블 시각화 뷰 (TableDataView)  struct TableDataView: UIViewRepresentable      AudioKit에서 제공하는 TableView를 UIKit 기반으로 감싸 SwiftUI에서 사용 가능하게 함   table을 주입받아 makeUIView에서 그려줌     📱 SwiftUI 구성 (TableRecipeView)  VStack {   Text(\"Square\")   TableDataView(table: conductor.square)   ... }      각 Table을 Text와 함께 나열하여 시각적으로 비교 가능     ✅ 요약                 생성 방식       설명                       .init(.sine, count: N)       기본형 파형 생성                 .init(file:)       오디오 파일 기반 파형                 .init(.harmonic, phase:)       배음 기반 사용자 정의 파형                 .init + 인덱스 수식       완전 커스텀 파형            "
  },
  
  {
    "title": "AudioKit 예제: Callback MIDI Instrument",
    "url": "/posts/CallbackMIDIInstrument/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-21 17:59:52 +0900",
    "content": "Callback MIDI Instrument     코드 보기   이 코드는 AudioKit + SwiftUI를 이용해 MIDI 시퀀서 이벤트를 감지하고 사운드폰트로 소리를 재생하며, 동시에 이벤트 로그를 실시간으로 출력합니다. 핵심 개념은 다음 세 가지입니다:     MIDICallbackInstrument: MIDI 이벤트를 실시간으로 감지하여 로그 출력   MIDISampler: MIDI 노트를 실제로 재생   AppleSequencer: 시간에 따라 MIDI 노트를 자동으로 발생시키는 시퀀서     클래스 구성: CallbackInstrumentConductor  주요 변수                 변수       설명                       engine       AudioKit 오디오 엔진                 sequencer       AppleSequencer, 박자/템포 기반으로 노트를 자동 재생                 callbacker       MIDI 이벤트를 감지하기 위한 콜백 인스트루먼트                 sampler       사운드폰트 기반으로 소리를 출력하는 샘플러                 tempo       템포 (BPM)                 division       한 마디 내 노트 수 (박자 세분화)                 text       MIDI 이벤트 발생 로그             초기화 흐름  init() {   setCallback()                             // 콜백 설정   try sampler.loadSoundFont(...)           // 사운드폰트 로드   _ = sequencer.newTrack()                 // 클릭 트랙 (이벤트 감지)   _ = sequencer.newTrack(\"sound\")          // 사운드 트랙 (소리 출력)   createClickTrack()                       // 노트 생성   sequencer.setTempo(...)                  // 템포 설정   engine.output = sampler                  // 출력 설정 }     createClickTrack(): 실제 트랙 생성     division 만큼 루프를 돌면서 노트를 삽입   clickTrack: callbacker에게 MIDI를 보내 이벤트 로그를 남김   soundTrack: sampler에게 MIDI를 보내 실제 소리 재생   clickTrack.setMIDIOutput(callbacker.midiIn) soundTrack.setMIDIOutput(sampler.midiIn)   노트는 아래 두 지점에 생성됩니다:     첫 박자 시작 (firstPosition)   중간 박자 위치 (secondPosition)     setCallback(): 콜백 로직  self.callbacker = MIDICallbackInstrument { ... }           .noteOn 발생 시:             현재 시퀀서 시간 (self.sequencer.currentPosition.seconds)과 노트 번호를 로그에 추가             🖥️ SwiftUI 뷰: CallbackInstrumentView  주요 UI 구성:                 UI 요소       설명                       Play, Pause, Rewind       시퀀서 제어                 CookbookKnob       템포 조절                 Slider       division 조절                 ScrollView + Text       콜백 로그 출력 (자동 스크롤 포함)           특징:     division을 변경하면 시퀀서를 멈추고 트랙을 재구성   로그는 아래처럼 출력됨:   Start Note 60 at 0.0000 Start Note 61 at 0.5000 ...      스크롤은 .scrollTo(\"logBottom\")으로 항상 아래로 유지됨     Preview  #Preview {   CallbackInstrumentView() }   Xcode의 canvas에서 인터랙티브 미리보기 지원    요약                 기능       구현 방식                       MIDI 노트 생성       AppleSequencer로 자동 반복 생성                 소리 재생       MIDISampler + 사운드폰트                 이벤트 감지       MIDICallbackInstrument                 로그 출력       SwiftUI Text + ScrollView             확장 아이디어     각 노트에 따라 다른 소리/색상 출력   외부 MIDI 입력과 연동   division이나 템포 변경 시 실시간 반영   .noteOff도 감지해 note duration 분석  "
  },
  
  {
    "title": "AudioKit의 Waveform",
    "url": "/posts/Waveform/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-20 20:10:19 +0900",
    "content": "Waveform     코드 보기   이 코드는 SwiftUI와 AudioKit, WaveformKit을 활용하여 오디오 파형을 시각화하고, 사용자가 드래그로 재생 범위(start ~ length)를 지정할 수 있는 인터랙티브한 Waveform 플레이어입니다. 각 구성요소에 대해 역할과 동작 방식을 중심으로 자세히 설명드리겠습니다.    1. WaveformConductor  class WaveformConductor: ObservableObject, ProcessesPlayerInput   🔹 역할     오디오 재생을 담당하는 AudioKit 기반 오디오 컨트롤러   AudioPlayer, SampleBuffer를 설정하고 제공   UI와 바인딩될 수 있도록 @Published를 통해 samples 공개   🔹 내부 구성     AudioEngine을 통해 오디오 재생 환경 설정   player는 AudioPlayer, .buffer에 Cookbook.sourceBuffer(source: .piano) 를 설정해 .piano 샘플을 사용   player.isLooping = true → 오디오 무한 반복        createSampleBuffer():             AudioBuffer를 Float 배열로 변환해 시각화를 위한 SampleBuffer 생성             2. WaveformView  struct WaveformView: View   🔹 역할     파형 시각화 및 드래그 UI 포함   start와 length를 조절해서 재생 범위 지정   실제 오디오 재생은 conductor를 통해 수행됨   🔹 구성 상태 변수                 변수       설명                       conductor       오디오 재생 관리                 start       선택된 구간의 시작 위치 (0.0 ~ 1.0)                 length       선택된 구간의 길이 (0.0 ~ 1.0)                 formatter       숫자 표시용 포맷터             내부 뷰 구성  ① 파라미터 표시  Text(\"start: \\(start), length: \\(length), end: \\(start + length)\")      현재 재생 범위 (0.0 ~ 1.0 단위) 표시   ② PlayerControlsII  PlayerControlsII(conductor: conductor, source: .piano) { conductor.createSampleBuffer() }      AudioKit Cookbook에서 제공하는 제어용 컨트롤 뷰   .piano 소스를 재생하며, 리셋 버튼 누르면 createSampleBuffer() 호출   ③ Waveform 파형 시각화 1 (큰 화면)  ZStack {   Waveform(samples: conductor.samples)   MinimapView(start: $start, length: $length) }      파형 위에 투명한 사각형(MinimapView)을 덮어 드래그 영역 지정 가능   ④ Waveform 파형 시각화 2 (선택 영역만 강조)  Waveform(   samples: conductor.samples,   start: Int(start * sampleRangeLength),   length: Int(length * sampleRangeLength) )      위에서 선택된 start ~ length 범위만 강조해서 보여줌   ⑤ 라이프사이클  .onAppear(perform: conductor.start) .onDisappear(perform: conductor.stop)      뷰가 등장하면 오디오 재생 시작, 사라지면 정지     3. MinimapView  🔹 역할     사용자가 드래그로 재생 범위를 선택할 수 있도록 하는 뷰        두 개의 RoundedRectangle이 있고:             하나는 선택된 영역       하나는 우측 경계 조절 핸들             주요 상태                 변수       설명                       @Binding var start       시작 위치 (WaveformView에서 바인딩)                 @Binding var length       길이                 @GestureState       드래그 시작 시 기준값 저장용 상태 변수             내부 구성  GeometryReader { geometry in   RoundedRectangle(...)    // 선택 영역   RoundedRectangle(...)    // 우측 조절 핸들 }   드래그 동작 처리  각 사각형에 .gesture(dragGesture(...))를 추가하여 아래와 같이 처리합니다:  func dragGesture(of mode: DragMode, geometryProxy geometry: GeometryProxy) -&gt; some Gesture                  DragMode       동작                       .selectedArea       사각형 전체를 좌우로 이동 (start 변경)                 .handle       우측만 드래그 → length 조절           드래그 비율 계산:  drag.translation.width / geometry.size.width   clamped(to:)로 start + length ≤ 1.0 제약 유지    실제 파형 재생 제어는?  현재 WaveformView에서 .onAppear(perform: conductor.start)로 전체 오디오가 루프 재생되며, 선택된 구간만 재생하도록 확장하려면 player.play(from:to:) 메서드로 start와 length 기반 구간 재생을 구현해야 합니다 (이전 답변 참고).    전체 흐름 요약  AudioPlayer.loadBuffer(.piano)         ↓ SampleBuffer 생성 → WaveformView에 연결         ↓ Waveform 시각화 + 드래그 제스처로 start/length 조절         ↓ 하단에 선택된 영역 Waveform 강조 표시         ↓ (.onAppear 시 전체 루프 재생)    "
  },
  
  {
    "title": "AudioKit의 SynthesisToolkit(STK)",
    "url": "/posts/SynthesisToolkit(STK)/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-20 16:05:35 +0900",
    "content": "Synthesis Toolkit (STK)     코드 보기   이 코드는 AudioKit, STKAudioKit, SwiftUI를 활용하여 구현된 셰이커(Shaker) 기반 메트로놈 데모 앱입니다. 사용자는 템포, 박자, 음색, 벨로시티 등을 조절할 수 있으며, 시각적인 피드백도 함께 제공합니다. 아래는 구조를 전체적으로 설명한 뒤, 구성 요소별로 자세하게 해석해드립니다.    🧭 전체 개요     STKView: 사용자 인터페이스를 담당하는 SwiftUI 뷰   ShakerConductor: AudioKit 엔진을 설정하고 Sequencer 및 Shaker를 관리하는 오디오 컨트롤러   SynthesisToolkitView.swift는 STKAudioKit 기반의 샘플 기반 악기인 Shaker를 사용한 오디오 타이머/메트로놈 구현입니다.     🎼 오디오 구성 (ShakerConductor)  class ShakerConductor: ObservableObject, HasAudioEngine   구성 요소:                 속성       설명                       shaker       STKAudioKit의 셰이커 악기                 callbackInstrument       비트 타이밍마다 호출되는 콜백 악기                 reverb       셰이커에 적용할 리버브                 sequencer       시퀀서 – 박자 및 타이밍 제어                 data       사용자 설정 상태 (isPlaying, tempo, currentBeat 등)           핵심 동작:     data가 바뀔 때마다 didSet에서 시퀀서 재시작 + 템포 반영 + 시퀀스 갱신   updateSequences()에서 박자에 따라 셰이커 노트 시퀀스를 생성   track.sequence.add(...) // 각 박에 음표 추가      callbackInstrument는 UI 업데이트용으로 박마다 콜백을 호출     🧱 SwiftUI 레이아웃 구성 (STKView)  🌐 Orientation-aware Layout  let isLandscape = geometry.size.width &gt; geometry.size.height      화면 비율을 기준으로 가로/세로 모드를 판별   각 모드에 맞는 HStack/VStack 레이아웃을 다르게 설정     ▶️ PlayButtonArea     “Start”/”Stop” 토글 버튼   GeometryReader를 통해 버튼을 가운데에 배치   .position(x: geometry.size.width / 2, y: geometry.size.height / 2)     🎚 TempoSliderArea  Slider(value: $conductor.data.tempo, in: 60.0...240.0)      60~240 BPM 범위 내에서 템포 조절 가능   텍스트로 현재 템포 표시     🎵 BeatSelectArea          두 개의 Stepper UI:             Downbeat (첫 박자에 나올 음색)       Other beats (나머지 박자의 음색)           conductor.data.downbeatNoteNumber += 1      MIDI 노트 번호를 기반으로 ShakerType과 매핑된 음색을 설정함     🔊 VelocityArea  Slider(value: $conductor.data.beatNoteVelocity, in: 0...127)      셰이커의 볼륨 (벨로시티) 조절     🧮 BeatCounterArea     현재 박자 수(timeSignatureTop)만큼 버튼이 생성   현재 박자는 data.currentBeat와 비교해 색상 강조   마지막 “+” 버튼을 눌러 박자 수 증가 가능   GeoCircleButton(...) { conductor.data.timeSignatureTop += 1 }     🟠 GeoCircleButton  GeometryReader { geometry in   let fontSize = size * 0.5      원형 버튼 안에 텍스트가 동적으로 크기 조절됨   .aspectRatio(1, .fit)로 정사각형 → 원 형태 유지     🔬 updateSequences()  track.sequence.add(noteNumber: data.downbeatNoteNumber, ...)      첫 번째 트랙에 메트로놈 셰이커 노트를 추가   두 번째 트랙(콜백 트랙)은 beat마다 CallbackInstrument 콜백 호출   currentBeat 값을 실시간으로 업데이트하여 UI와 동기화     📈 FFTView  FFTView(conductor.reverb)      AudioKitUI의 실시간 주파수 스펙트럼 뷰   reverb로 출력된 오디오를 시각화     🧪 Preview  #Preview {   STKView() }      Xcode canvas에서 실시간 인터랙션 테스트 가능     ✅ 요약                 구성 요소       역할                       ShakerConductor       오디오 엔진, 시퀀서, 셰이커 구성                 STKView       SwiftUI 기반 UI                 .onAppear/.onDisappear       오디오 엔진 시작/종료                 updateSequences()       박자에 따라 음 시퀀스 자동 구성                 GeoCircleButton       박자 표시용 원형 버튼 (터치 가능)                 CallbackInstrument       시퀀서와 UI를 연결하는 핵심 콜백                 FFTView       실시간 시각화            "
  },
  
  {
    "title": "AudioKit의 Flow",
    "url": "/posts/Flow/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-19 16:03:22 +0900",
    "content": "Flow     코드 보기   이 코드는 SwiftUI와 Flow 라이브러리를 이용해 모듈형 오디오 또는 비주얼 그래프 편집기 인터페이스를 구현한 예제입니다. 사용자는 “Simple” 또는 “Random” 패치 구성을 선택하여 그래프 형태로 노드(Node)와 와이어(Wire)를 시각적으로 다룰 수 있습니다.    🧠 핵심 클래스: FlowConductor  ObservableObject를 채택한 뷰 모델로서, FlowView에서 사용됩니다.  🔸 주요 프로퍼티     patch: Patch: 현재 그래프 구조 (노드 + 와이어 집합)   selection: Set&lt;NodeIndex&gt;: 사용자가 현재 선택한 노드들   segIndex: Int: “Simple” 또는 “Random” 구성을 고르는 Segment Picker의 선택 인덱스   🔸 init()     뷰가 처음 생성될 때 simplePatch()를 호출해 기본 구성의 패치를 설정     🔧 simplePatch() 함수  간단한 오디오/비주얼 흐름 예제를 구성합니다:          노드 생성             generator, processor, mixer, output 총 6개 노드를 생성       generator와 processor 각각 두 개씩 → 총 6개 노드       각 노드는 이름, 입력/출력 포트, 색상 정보를 가짐                와이어 연결             Wire(from: OutputID, to: InputID) 형태로 노드 간 연결       각 와이어는 출력 포트 → 입력 포트를 의미           예:      Wire(from: OutputID(0, 0), to: InputID(1, 0))           → 첫 번째 generator(노드 0)의 출력이 첫 번째 processor(노드 1)의 입력으로 연결됨           배치 레이아웃 설정             마지막 노드(출력 노드, 인덱스 5)를 기준 위치 (800, 50)에 고정       recursiveLayout을 통해 나머지 노드들이 자동 배치됨                self.patch = patch로 최종 구성 반영        🔀 randomPatch() 함수     무작위 노드 50개와 와이어 50개를 생성하여 복잡한 그래프 구성 예제를 제공   각 노드는 랜덤한 위치 및 색상   각 와이어는 랜덤하게 다른 노드와 연결됨   이 함수는 segIndex가 1일 때 실행됩니다.    🖼️ FlowView: SwiftUI View  구조:  VStack {   Picker(...)        // \"Simple\" 또는 \"Random\" 선택   NodeEditor(...)    // Flow 라이브러리의 메인 편집기 뷰 }      .onAppear: 화면이 나타날 때 가로모드 강제 전환   .onDisappear: 뷰가 사라질 때 방향 제한 해제   Picker(\"Select the Patch\", selection: $conductor.segIndex)     사용자가 선택한 값(segIndex)에 따라 simplePatch() 또는 randomPatch()가 자동 호출됨     🧭 NodeEditor(...)     Flow 라이브러리의 메인 UI로, 노드와 와이어를 시각적으로 편집할 수 있게 하는 컴포넌트   $conductor.patch, $conductor.selection과 바인딩되어 실시간 편집 가능     🧪 기타     #Preview: SwiftUI Preview용 코드   forceOrientation(...): 가로모드로 화면 강제 전환 (별도 구현되어야 함)     ✅ 요약                 구성 요소       역할                       FlowConductor       노드 및 연결 정보를 관리하는 뷰 모델                 simplePatch()       미리 정의된 간단한 노드 구성 설정                 randomPatch()       50개의 랜덤 노드 및 연결 구성                 NodeEditor       노드 기반 UI를 표시하고 편집 가능하게 하는 뷰                 segIndex       SegmentPicker로 사용자가 선택한 구성 판단                 .onAppear       뷰 진입 시 가로모드 강제             Simple Patch 동작 해석  Flow 라이브러리에서 Wire는 두 노드를 연결하는 선을 의미합니다. 실제로는 한 노드의 출력 포트(OutputID)가 다른 노드의 입력 포트(InputID)로 연결되는 구조입니다.  let wires = Set([   Wire(from: OutputID(0, 0), to: InputID(1, 0)), // gen 1 -&gt; proc 1   Wire(from: OutputID(1, 0), to: InputID(4, 0)), // proc 1 -&gt; mixer   Wire(from: OutputID(2, 0), to: InputID(3, 0)), // gen 2 -&gt; proc 2   Wire(from: OutputID(3, 0), to: InputID(4, 1)), // proc 2 -&gt; mixer   Wire(from: OutputID(4, 0), to: InputID(5, 0)), // mixer -&gt; output ])   이제 이걸 기반으로 Wire 간의 동작 흐름을 실제 예제로 해석해보겠습니다.    🎯 노드 구성 요약 (nodes 배열 순서)                 인덱스       노드 이름       타입       입력       출력                       0       generator       source       없음       out                 1       processor       effect       in       out                 2       generator       source       없음       out                 3       processor       effect       in       out                 4       mixer       utility       in1, in2       out                 5       output       sink       in       없음             🔗 Wire 흐름 예제 해석  1. Wire(from: OutputID(0, 0), to: InputID(1, 0))     generator 1 → processor 1   generator 1의 첫 번째 출력(\"out\") → processor 1의 첫 번째 입력(\"in\")   ▶️ 첫 번째 오디오 소스가 이펙터로 들어감     2. Wire(from: OutputID(1, 0), to: InputID(4, 0))     processor 1 → mixer (in1)   processor 1 출력 → mixer의 첫 번째 입력(\"in1\")   ▶️ 이펙트 처리된 소리가 믹서로 들어감     3. Wire(from: OutputID(2, 0), to: InputID(3, 0))     generator 2 → processor 2   두 번째 소스 → 두 번째 이펙터     4. Wire(from: OutputID(3, 0), to: InputID(4, 1))     processor 2 → mixer (in2)   ▶️ 이펙트 처리된 두 번째 소리가 믹서로 들어감     5. Wire(from: OutputID(4, 0), to: InputID(5, 0))     mixer → output   믹서의 결과가 최종 출력 노드로 이동     📊 시각적 흐름 요약 (왼쪽 → 오른쪽)  gen1 → proc1 ┐              ├→ mixer → output gen2 → proc2 ┘      두 개의 소스 → 각자 이펙트 처리 → 믹서로 병합 → 최종 출력   이 구조는 이중 채널 처리, 병렬 이펙팅 후 믹싱 등의 오디오 워크플로우와 유사합니다     🧩 실제 활용 시 예시 (AudioKit 연결 예)                 노드       연결 대상 (AudioKit)                       generator       Oscillator() 또는 Player()                 processor       LowPassFilter, Reverb 등                 mixer       Mixer()                 output       engine.output = ...             ✅ 정리                 Wire 구성       의미                       OutputID(a, x)       a번 노드의 x번째 출력 포트                 InputID(b, y)       b번 노드의 y번째 입력 포트                 Wire(from: ..., to: ...)       a번 노드 → b번 노드 간 연결선           이 구조는 실제 오디오 시그널 플로우(신호 흐름)를 시각화하거나, 실시간 처리 체인을 구성할 때 매우 직관적입니다.   "
  },
  
  {
    "title": "AudioKit의 MultiSegmentPlayer",
    "url": "/posts/MultiSegmentPlayer/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-18 19:11:56 +0900",
    "content": "MultiSegment Player     코드 보기   이 코드는 AudioKit을 기반으로 여러 오디오 파일을 연속 재생하며, 실시간 RMS 기반 시각화와 플레이헤드를 포함한 타임라인 UI를 보여주는 멀티 세그먼트 오디오 플레이어 구현입니다.  MultiSegmentPlayerConductor는 오디오 제어 로직을 담당하고, MultiSegmentPlayerView는 이를 화면에 표시합니다.    🧠 전체 구조 개요     ✅ MultiSegmentPlayerConductor: 오디오 재생, 시간 추적, RMS 설정, 세그먼트 생성, 오디오 세션 구성 등 핵심 로직   ✅ MultiSegmentPlayerView: 시각적 UI (파형, 플레이헤드, 버튼 등)   ✅ RMS: 소리의 실제 감지 크기를 표현하는 지표 (RMS = Root Mean Square)     🔷 MultiSegmentPlayerConductor 클래스  1. 오디오 구성 요소     engine: AudioKit의 오디오 엔진   player: 세그먼트 기반의 오디오 플레이어 (여러 오디오 파일 재생 가능)   2. 시간 관련 변수     timer: 0.05초 주기로 checkTime() 호출   timePrevious: 이전 시간 저장 (현재 시간과 비교해 delta time 계산)   timestamp: 재생 시간 누적값 (@Published로 UI와 바인딩됨)   endTime: 전체 오디오 길이   3. RMS 시각화 제어  RMS (Root Mean Square) RMS (Root Mean Square)는 신호의 전반적인 에너지 크기(=지속적인 평균적인 세기)를 나타내는 통계적 측정값입니다. 오디오에서는 “소리의 실제 감지되는 볼륨 크기”에 가까운 값으로 간주됩니다.    오디오 파형은 시간에 따라 위아래로 진동하는 파형인데, 단순 평균(mean)은 0이 되기 때문에 사용이 어렵습니다.   대신, 각 샘플의 제곱 → 평균 → 제곱근을 구해 전체적인 크기를 양수로 계산하는 것이 RMS입니다.   RMS는 Peak보다 낮지만 실제 듣는 소리의 크기와 유사합니다.   변수 설명    rmsFramePerSecond: 1초에 몇 개의 RMS 샘플을 만들지 결정 → 높을수록 부드러운 파형   pixelsPerRMS: RMS 하나당 몇 픽셀 차지할지 → 시각화의 밀도 제어   4. 재생 상태 (isPlaying)     true: 시간 초기화 후 player.playSegments() 호출 → 세그먼트 순차 재생 시작   false: player.stop() → 재생 중지     5. 주요 함수     currentUptimeSeconds(): 시스템 부팅 이후 경과 시간(초 단위) 계산   checkTime(): 매 프레임마다 시간 흐름 계산하여 timestamp 업데이트   createSegments(): 여러 오디오 파일을 시간차를 두고 연속 배치   setEndTime(): 마지막 세그먼트의 끝 시간을 기준으로 총 길이 설정   setAudioSessionCategoriesWithOptions(): 오디오 세션 설정 (스피커, 블루투스 등)   startAudioEngine(): AudioKit 엔진 시작   setTimer(): 타이머 시작     🔶 MultiSegmentPlayerView 구조체  구성 요소     TrackView: RMS 기반 파형을 그리는 커스텀 뷰   Rectangle(): 플레이헤드 (현재 재생 위치를 나타내는 빨간 선)   PlayPauseButton: 재생/일시정지 버튼 (아이콘 변경)   currentTimeText: 현재 시간과 총 시간 표시 (\"2.4 of 4.7\" 형식)   핵심 연산 프로퍼티  var currentPlayPosition: CGFloat {   let pixelsPerSecond = conductor.pixelsPerRMS * conductor.rmsFramePerSecond   return conductor.timestamp * pixelsPerSecond - playheadWidth }      timestamp를 픽셀 단위 위치로 환산 → 플레이헤드를 왼쪽으로 이동시킴   playheadWidth만큼 왼쪽으로 보정해서 중앙 정렬     🔧 실행 흐름 요약     onAppear 시 conductor.start() → 오디오 엔진 시작   Play 누르면 isPlaying = true → player.playSegments() 실행 + timestamp 시작   Timer가 checkTime()을 주기적으로 호출 → timestamp 증가   timestamp 값에 따라 플레이헤드 위치 및 시간 텍스트 실시간 갱신   RMS 파형은 TrackView에서 표시됨 (코드 미포함)     🎯 이 코드가 하는 일     다수의 오디오 파일을 시간차를 두고 이어붙여 순차 재생   RMS 기반 시각화 구현을 위한 값 설정   재생 시간에 따라 플레이헤드를 이동   SwiftUI UI와 오디오 재생이 실시간으로 연동    "
  },
  
  {
    "title": "음향에서 ADSR(Attack, Decay, Sustain, Release)란 무엇인가?",
    "url": "/posts/ADSR/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-16 18:17:27 +0900",
    "content": "ADSR  ADSR은 음향에서 **소리의 시간적 변화(Envelope)**를 모델링하기 위한 네 가지 기본 단계를 나타냅니다. 이는 특히 신디사이저, 샘플러, 효과기에서 음의 길이와 강약 변화를 제어하는 데 사용됩니다.    🔤 ADSR: 네 가지 단계                 약어       의미       설명                       A       Attack       건반을 누른 순간부터 최대 볼륨까지 도달하는 데 걸리는 시간                 D       Decay       Attack 이후, 소리가 최대치에서 **지속 볼륨(Sustain)**까지 줄어드는 시간                 S       Sustain       키를 누르고 있는 동안 유지되는 지속적인 볼륨 (시간이 아닌 레벨)                 R       Release       키를 뗀 후, 소리가 완전히 사라질 때까지 걸리는 시간             🎧 각 단계의 실제 효과  🟢 Attack (어택)     작을수록: 소리가 바로 나옴 (펀치감, 퍼커션류에 적합)   클수록: 소리가 점점 커지며 시작 (스트링, 패드 사운드에 적합)   🟠 Decay (디케이)     최대 볼륨에서 Sustain 레벨까지 떨어지는 시간   드럼처럼 빠르게 죽는 소리는 짧은 Decay, 피아노처럼 감쇠되는 소리는 중간 Decay 사용   🔵 Sustain (서스테인)     소리를 누르고 있는 동안 유지되는 볼륨 수준   0에 가까울수록 짧게 끝나는 느낌 / 1에 가까우면 지속됨   🔴 Release (릴리즈)     키를 놓은 뒤 소리가 사라지는 데 걸리는 시간   짧을수록 갑자기 꺼지는 느낌 / 길수록 여운이 남음 (리버브처럼)     🧠 ADSR은 왜 중요한가?     음색 컨트롤의 핵심 요소입니다.        같은 악기 소스도 ADSR 설정에 따라 전혀 다른 감성을 가질 수 있습니다.             빠른 Attack + 짧은 Release → 드럼       느린 Attack + 긴 Release → 스트링 패드           신디사이저에서는 필터나 볼륨 뿐만 아니라 피치, 모듈레이션 등 다양한 파라미터에 ADSR을 연결할 수 있습니다.     📊 시각적 예 (타임라인 흐름)  볼륨 │       /‾‾‾‾‾‾‾‾‾ │      /         \\ │     /           \\ │    /             \\__________ │                            \\ │                             \\________ │________________________________________ 시간      ↑A    ↑D             ←S→     ↑R     🎯 요약     ADSR은 소리의 생성과 사라짐을 시간적으로 컨트롤하는 4단계   악기의 캐릭터, 감정, 반응성을 정하는 중요한 요소   Attack/Decay/Release는 시간, Sustain은 볼륨 레벨  "
  },
  
  {
    "title": "AudioKit에서 필터(Filter)의 노드목록",
    "url": "/posts/%ED%95%84%ED%84%B0(Filter)_%EB%85%B8%EB%93%9C%EB%AA%A9%EB%A1%9D/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-15 23:38:56 +0900",
    "content": "Filter의 Node 목록    🔵 AudioKit 필터  🎛 HighPassFilter     기능: 지정된 컷오프 주파수 이하의 저역을 제거하고 고역을 통과시킴.        Cutoff Frequency             필터가 작동을 시작하는 기준 주파수       값이 높을수록 더 많은 저역이 제거됨                Resonance             컷오프 근처의 주파수를 강조하는 정도       값이 높을수록 해당 경계가 날카롭고 뚜렷하게 들림             🎛 HighShelfFilter     기능: 특정 주파수 이상 대역을 증폭 또는 감쇠 (고역 쉘프 필터)        Cut Off Frequency             고역 증감이 시작되는 지점                Gain             해당 대역을 얼마나 키울지 또는 줄일지       양수: 고역 강조 / 음수: 고역 약화             🎛 LowPassFilter     기능: 컷오프 주파수 이상을 제거하고 저역을 통과시킴        Cutoff Frequency             고역 차단 시작 지점       값이 낮을수록 더 많은 고역이 제거됨                Resonance             컷오프 주파수 부근을 얼마나 강조할지       값이 클수록 더 뾰족한 음색을 생성             🎛 LowShelfFilter     기능: 특정 주파수 이하의 저역을 증폭 또는 감쇠        Cutoff Frequency             저역 증감이 시작되는 기준 주파수                Gain             양수일 경우 저음 강화 / 음수일 경우 저음 감소             🔷 SoundpipeAudioKit 필터  🎛 BandPassButterworthFilter     기능: 특정 대역만 통과시키고 그 외는 차단 (부드러운 버터워스 특성)        Center Frequency             통과시킬 중심 주파수                Bandwidth             통과 대역폭 (넓을수록 많은 대역 허용)             🎛 BandRejectButterworthFilter     기능: 특정 대역만 제거하고 그 외는 통과 (노치 필터)        Center Frequency             제거 대상 중심 주파수                Bandwidth             제거 대역폭 (값이 클수록 넓은 영역이 제거됨)             🎛 EqualizerFilter     기능: 특정 대역만 조정 가능한 기본 이퀄라이저        Center Frequency             조절할 중심 주파수                Bandwidth             영향을 주는 대역폭 범위                Gain             해당 대역을 증폭(+)/감쇠(-) 정도             🎛 FormantFilter     기능: 인간 목소리의 공명 구조(포먼트)를 모방해 로봇 보이스나 보컬 이펙트에 활용        Center Frequency             포먼트 중심 주파수                Attack Duration             필터 적용 시 올라가는 속도 (빠를수록 즉각적 반응)                Decay Duration             효과가 사라지는 속도 (느릴수록 잔향 느낌)             🎛 HighPassButterworthFilter     기능: 저역을 부드럽게 제거하는 고역 필터        Cutoff Frequency             저역 차단 시작 지점 (값이 높을수록 더 많은 저역 제거)             🎛 HighShelfParametricEqualizerFilter     기능: 고역대 쉘프 필터에 정밀 제어를 더한 필터        Corner Frequency             고역 증감이 시작되는 주파수                Gain             고역을 얼마나 증폭하거나 감쇠할지                Q             변화가 일어나는 영역의 폭 (값이 작을수록 완만)             🎛 KorgLowPassFilter     기능: Korg 스타일의 필터로, 아날로그 특성 및 왜곡 포함        Filter Cutoff             고역 차단 시작점                Resonance             컷오프 부근 강조 정도                Saturation             왜곡량 조절 (값이 높을수록 따뜻하고 거친 소리)             🎛 LowPassButterworthFilter     기능: 부드럽게 고역을 제거하는 저역 필터        Cutoff Frequency             고역 차단 시작 지점             🎛 LowShelfParametricEqualizerFilter     기능: 저역대 쉘프 필터 + Q 제어 추가        Corner Frequency             저역 증감이 시작되는 주파수                Gain             저역 증폭/감쇠 정도                Q             변화 범위의 폭             🎛 ModalResonanceFilter     기능: 특정 주파수를 공명시켜 금속/현악 느낌을 부여        Resonant Frequency             공명 중심 주파수                Quality Factor             공명의 날카로움 (값이 높을수록 뾰족하고 긴 울림)             🎛 MoogLadder     기능: Moog 신시사이저의 Ladder 필터 모델링, 따뜻한 아날로그 느낌        Cutoff Frequency             고역 차단 지점                Resonance             컷오프 근처 강조 (값이 높을수록 더 신디사이저 느낌)             🎛 PeakingParametricEqualizerFilter     기능: 특정 대역만 강조하거나 감쇠 (중심 대역 이퀄라이저)        Center Frequency             조절할 중심 주파수                Gain             해당 대역 증폭/감쇠 정도                Q             영향받는 대역의 넓이             🎛 ResonantFilter     기능: 특정 주파수를 공명시키고 나머지는 억제        Frequency             공명 주파수                Bandwidth             공명 폭 (넓을수록 효과가 부드러움)             🎛 ThreePoleLowpassFilter     기능: 3단계 구조의 저역 필터, 왜곡 포함        Distortion             추가적인 음색 변형                Cutoff Frequency             고역 차단 시작점                Resonance             공명 강조 정도             🎛 ToneComplementFilter     기능: ToneFilter의 보완적 역할을 하는 필터        Half-Power Point             주파수 응답이 절반으로 줄어드는 기준 지점             🎛 ToneFilter     기능: 고역/저역에 일정한 감쇠 효과를 줘서 톤 정리        Half-Power Point             필터가 가장 많이 작용하는 중심 지점            "
  },
  
  {
    "title": "음향에서 필터(Filter)란?",
    "url": "/posts/%ED%95%84%ED%84%B0(Filter)/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-15 23:38:13 +0900",
    "content": "필터 (Filter)  음향에서의 **필터(Filter)**란, 오디오 신호에서 특정 주파수 성분을 통과시키거나 제거하는 처리기법입니다. 필터는 디지털 또는 아날로그 방식 모두로 구현될 수 있으며, 오디오에서 매우 중요한 역할을 합니다. 예를 들어, 원하는 음색을 만들거나, 잡음을 제거하거나, 공간감이나 특정 질감을 추가하는 데 사용됩니다.    📘 필터의 기본 개념  1. 주파수(Frequency)     소리는 다양한 주파수의 조합으로 이루어집니다.   예: 저음(20250Hz), 중음(2504kHz), 고음(4kHz~20kHz)   2. 필터의 주요 기능     특정 **주파수 대역을 통과(Pass)하거나 제거(Cut)**합니다.   3. 필터의 기본 분류                 필터 종류       기능 설명                       Low-Pass Filter       특정 주파수보다 낮은 주파수만 통과, 고주파 제거                 High-Pass Filter       특정 주파수보다 높은 주파수만 통과, 저주파 제거                 Band-Pass Filter       특정 대역만 통과하고 나머지 제거                 Band-Reject Filter (Notch)       특정 대역만 제거하고 나머지 통과                 Shelf Filter       특정 주파수 이상/이하를 점진적으로 증감 (boost/cut)                 Parametric EQ       중심 주파수, 폭(Q), 증감량(gain)을 조절해 특정 주파수 대역 조절                 Formant Filter       사람 목소리의 공명 주파수를 강조 – 보컬/로봇 보이스 등                 Resonant Filter       특정 주파수를 강조하여 공명 효과 유도             🔍 공통적으로 학습해야 할 핵심 개념  각 필터 종류는 구현 방식이나 특성은 다르지만, 다음의 공통 요소를 갖고 있어 이를 먼저 이해하는 것이 중요합니다:  1. Cutoff Frequency (컷오프 주파수)     필터가 작용하기 시작하는 기준 주파수   대부분의 필터가 이 파라미터를 가짐   2. Resonance / Q (품질 계수)     필터 가장자리에 얼마나 강하게 강조 또는 감쇠할지   값이 높을수록 특정 주파수에서 공명(peaking) 효과가 큼   3. Gain (이득)     해당 대역을 얼마나 증폭하거나 줄일 것인지 (특히 EQ에서)   4. Bandwidth (대역폭)     중심 주파수를 기준으로 얼마나 넓은 영역을 처리할지     📚 예시를 통한 필터별 비교                 필터명       주요 파라미터       설명                       BandPassButterworthFilter       centerFrequency, bandwidth       중심 대역만 통과 (버터워스 방식으로 급격하게 차단)                 BandRejectButterworthFilter       centerFrequency, bandwidth       중심 대역만 제거                 EqualizerFilter       gain, centerFrequency       기본 EQ, 단일 대역 증감                 FormantFilter       frequency, attackDuration       음성 성대 필터 특성 모방                 HighPassFilter, LowPassFilter       cutoffFrequency, resonance       단순 고역/저역 필터                 HighShelfFilter, LowShelfFilter       gain, cutoffFrequency       특정 주파수 이상/이하 전 대역 증감                 MoogLadder       cutoffFrequency, resonance       아날로그 신디사이저의 따뜻한 느낌 재현                 PeakingParametricEqualizerFilter       centerFrequency, gain, Q       중심 주파수를 중심으로 양방향 증감 가능             ✅ 실습 또는 학습 팁          Dry/Wet Mix로 차이 들어보기             필터 전/후의 차이를 듣는 것이 가장 중요합니다.       DryWetMixer로 비교하면 훨씬 이해가 빠릅니다.                하나씩 파라미터를 조절해보기             Frequency, Gain, Q를 바꿔가며 효과 확인                시각적 분석도 활용             FFTPlot, NodeOutputPlot 같은 시각 도구로 필터 효과 확인                실제 음악에 적용해보기             드럼, 보컬, 신디 등 다양한 소스에 필터를 걸어 청감 실험             🎯 요약                 핵심 키워드       설명                       필터(Filter)       특정 주파수 성분을 제거/강조하는 처리 도구                 주파수(Frequency)       음향의 기본 구성 요소, 필터의 기준                 Cutoff / Center Frequency       필터가 작용하는 지점                 Q / Resonance       얼마나 날카롭고 강조되는지                 Gain       특정 주파수 대역을 얼마나 키우거나 줄일지            "
  },
  
  {
    "title": "AudioKit의 PeakLimiter",
    "url": "/posts/PeakLimiter/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-08 16:22:29 +0900",
    "content": "Peak Limiter  🎚️ 피크 리미터(Peak Limiter)란?  피크 리미터는 오디오 신호의 최대 진폭(peak amplitude)을 강제로 제한(clamp) 하여 지정된 레벨 이상으로 신호가 넘어가지 않도록 막는 효과입니다. 보통 다음과 같은 상황에 사용됩니다:     🔊 클리핑 방지: 신호가 너무 커서 디지털 클리핑이 발생하는 것을 막기 위해   🎤 라이브 입력: 마이크나 악기처럼 예상할 수 없는 큰 소리가 들어올 때   📼 마스터링: 오디오 트랙의 피크를 일정 수준으로 제한해 과도한 출력 방지     🔧 주요 파라미터 설명  Attack Time | 0.012 | 0.0005...0.03 Decay Time  | 0.024 | 0.001...0.04 Pre Gain    | 0.0   | -40.0...40.0   1. 🕒 Attack Time     의미: 입력 신호가 임계값을 초과했을 때 리미팅을 시작하는 데 걸리는 시간 (초)   짧을수록: 빠르게 반응해 순간적인 피크도 잘 잘라냅니다.   길수록: 부드럽게 반응하지만, 빠른 트랜지언트(예: 킥 드럼)는 놓칠 수 있습니다.                  값       효과 설명                       0.0005       극단적으로 빠르게 반응                 0.012       일반적인 설정 (기본값)                 0.03       느리게 반응, 더 자연스러움             2. 🕓 Decay Time     의미: 피크 리미터가 동작한 후 원래 상태로 회복되는 시간 (초)   짧을수록: 리미팅이 빠르게 해제됨 → 소리가 튀듯 들릴 수 있음   길수록: 더 부드럽고 자연스럽게 해제됨 → 지속적인 리미팅 효과                  값       효과 설명                       0.001       빠르게 원상 복구됨                 0.024       일반적인 설정 (기본값)                 0.04       자연스럽고 부드럽게 복원됨             3. 🔊 Pre Gain     의미: 리미터 전에 신호를 증폭 or 감쇠        용도:             음량이 작은 소스를 리미터 수준까지 끌어올릴 때       리미팅 테스트를 위해 신호를 강제로 클리핑 수준까지 올릴 때           양수 값: 리미터가 더 자주, 더 강하게 작동함   음수 값: 신호가 약해져 리미터가 잘 작동하지 않음                  값       효과                       -40.0       거의 리미팅 작동 안 함                 0.0       원래 볼륨                 +10.0       피크 리미팅 자주 발생                 +40.0       매우 자주 리미팅 발생 (강제)             📈 시각적 예시  원래 신호:      ────────▇▇▇▇▇──────────────  리미터 적용 후: ────────█████──────────────  ← 일정 레벨 이상 깎임     참고: 리미터와 컴프레서의 차이  일반적으로 PeakLimiter는 “어느 볼륨부터 리미트를 걸지”에 대한 명시적인 임계값(threshold) 설정이 없습니다. 대신, 내부적으로 고정된 threshold 값을 기준으로 동작합니다. 즉:     PeakLimiter는 사용자가 threshold(임계값)를 직접 설정하지 않아도,   내부에서 적절한 임계값(예: -0.1dBFS, -0.3dBFS 등)을 기준으로 판단하여,   그 이상으로 올라가는 피크에 자동으로 리미트(limiting)를 적용합니다.   📌 리미터 vs 컴프레서 비교                 구분       PeakLimiter       Compressor                       Threshold       사용자가 설정 못함 (내장값 사용)       사용자가 직접 설정함                 Ratio       매우 높음 (∞:1)       다양하게 설정 가능 (예: 2:1, 4:1)                 용도       피크 급락 방지, 클리핑 방지       음량 균형 조절, 다이내믹 톤 연출                 적용 시점       마스터링, 라이브 입력 안정화       개별 트랙 조정, 믹싱 단계 등           💡 내부 임계값이란?  AudioKit의 PeakLimiter는 Apple의 AVAudioUnitPeakLimiter를 감쌌으며, 해당 Apple 문서에서는 threshold 설정이 없고 다음처럼 설명됩니다:     “This effect prevents clipping by automatically limiting the amplitude of audio that exceeds a certain internal threshold. (이 효과는 특정 내부 임계값을 초과하는 오디오의 진폭을 자동으로 제한하여 클리핑을 방지합니다.)”   즉, 사용자가 직접 어느 볼륨부터 자를지는 못 정하고, 리미터가 자동으로 판단해 적용하는 구조입니다.    ✅ 정리                 요소       설명                       PeakLimiter       최대 신호 진폭을 제한하여 클리핑 방지                 Attack Time       리미터가 작동을 시작하는 속도                 Decay Time       리미터가 원상복귀하는 속도                 Pre Gain       리미터 전에 신호 증폭/감쇠                 활용 상황       라이브 입력, 마스터링, 과도한 피크 방지 등           이 리미터는 특히 예상할 수 없는 입력의 최대 음량을 안전하게 제한할 때 유용합니다. "
  },
  
  {
    "title": "AudioKit의 MultiTapDelay",
    "url": "/posts/MultiTapDelay/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-07 14:53:20 +0900",
    "content": "Multi Tap Delay     코드 보기   MultiTapDelayConductor 클래스는 AudioKit을 사용하여 멀티탭 딜레이(Multi-Tap Delay) 효과를 구현하는 Swift 클래스입니다. 여러 개의 지연된 복사본을 입력 신호에 섞어 리듬감 있는 반복 효과나 리버브와 유사한 풍부한 공간감을 만들어냅니다.    📌 클래스 설명  class MultiTapDelayConductor: ObservableObject, ProcessesPlayerInput     ObservableObject: SwiftUI 뷰와 데이터를 바인딩하기 위해 사용됩니다. 슬라이더를 통해 딜레이 시간 및 게인 값이 바뀌면 뷰가 자동으로 업데이트됩니다.   ProcessesPlayerInput: 오디오 재생 처리를 담당하는 AudioKit Cookbook의 프로토콜입니다.     🔧 주요 변수                 변수 이름       설명                       engine       AudioKit의 오디오 엔진                 player       AudioPlayer: 오디오 파일을 반복 재생합니다.                 buffer       오디오 파일을 메모리에 로드한 버퍼. 음원 데이터 저장                 defaultSource       초기 재생할 오디오 파일. 여기서는 .femaleVoice                 delays       VariableDelay 배열. 각 탭의 지연 요소입니다.                 faders       Fader 배열. 각 탭의 게인 조절기입니다.                 times       슬라이더로 조절할 각 탭의 딜레이 시간 ([0.1, 0.2, 0.4]초)                 gains       각 탭의 출력 음량 ([0.5, 2.0, 0.5])             🎛️ init()     player.buffer에 오디오 데이터를 세팅하고 반복 재생을 설정합니다.   engine.output에 multiTapDelay(player, times: times, gains: gains)을 설정하여 엔진 출력에 멀티탭 딜레이 효과를 적용합니다.     🧩 multiTapDelay(_ input: Node, times: [AUValue], gains: [AUValue]) -&gt; Mixer     **입력 노드(input)**의 신호를 딜레이 + 게인 조절해서 믹서에 추가합니다.   딜레이와 게인은 슬라이더로 개별 조정 가능.   Mixer에 원본 입력도 함께 포함되어 Dry/Wet 믹싱 효과가 생깁니다.   for (i, (time, gain)) in zip(times, gains).enumerated() {   delays.append(VariableDelay(input, time: time))   faders.append(Fader(delays[i], gain: gain))   mix.addInput(faders[i]) }   즉, VariableDelay → Fader → Mixer 순으로 체인 구성합니다.    🔄 updateDelays()     슬라이더 값이 변경될 때마다 호출되어 딜레이 및 게인 값을 실시간 반영합니다.   delays[i].time = times[i] faders[i].gain = gains[i]     ✅ 요약                 구성 요소       설명                       🎧 딜레이 타임       [0.1, 0.2, 0.4]초로 지정된 여러 지점에서 입력 신호를 지연                 🎚 게인       각 딜레이된 신호의 볼륨을 개별 조절                 🎼 사용 효과       리듬감 있는 에코, 다층적인 반향, 리버브 유사 환경                 🔁 슬라이더 연동       @Published 값이 변할 때 updateDelays() 호출로 실시간 반영             📊 오디오 체인 구성도  player   │   ├──▶ VariableDelay(time: 0.1) ─▶ Fader(gain: 0.5) ─┐   ├──▶ VariableDelay(time: 0.2) ─▶ Fader(gain: 2.0) ─┤   ├──▶ VariableDelay(time: 0.4) ─▶ Fader(gain: 0.5) ─┤   └──────────────────────────────────────────────────▶ Mixer (최종 출력)     이 구조는 동일한 소스에서 여러 딜레이 시점을 만든 다음, 볼륨 조절 후 믹싱하는 구조로, 공간감 있고 풍부한 사운드를 만드는 데 매우 유용합니다. "
  },
  
  {
    "title": "AudioKit의 Flanger",
    "url": "/posts/Flanger/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-06 13:52:59 +0900",
    "content": "Flanger  Flanger는 오디오 이펙트 중 하나로, 짧은 시간 지연(delay)과 지속적인 변조(modulation)를 이용해 스윙감이 있는 웅웅거리는 소리를 만들어내는 효과입니다. 일종의 위상 간섭을 활용하여 음원에 움직임을 부여합니다.    🎛️ Flanger의 역할     지연된 신호를 원래 신호에 더해서 두 신호 간의 위상 차이로 생기는 콤 필터(Comb Filter) 효과를 이용합니다.   이 지연 시간은 LFO (Low Frequency Oscillator)로 주기적으로 변화하며, 이로 인해 움직이는 듯한 “우웅~ 우웅~” 소리가 납니다.   전기 기타, 신디사이저, 드럼, 보컬 등 다양한 소스에 사용됩니다.     🧪 주요 파라미터 설명  1. Frequency (Hz)     범위: 0.1 ~ 10.0   기본값: 1.0   설명: 지연 시간에 변화를 주는 LFO의 진동수(속도)입니다. 즉, 플랜저의 움직임이 몇 초에 한 번 반복되는지 결정합니다.                  값       의미                       낮은 값 (0.1 ~ 1)       느린 스윙, 부드러운 움직임                 높은 값 (5 ~ 10)       빠른 진동, 트레몰로처럼 들릴 수 있음             2. Depth (0~1)     범위: 0.0 ~ 1.0   기본값: 0.25   설명: LFO가 지연 시간을 얼마나 변화시키는지, 즉 변조의 깊이입니다.                  값       효과                       낮은 값       미세한 움직임, subtle한 효과                 높은 값       강한 모듈레이션, 휘청거리는 소리             3. Feedback (-0.95 ~ 0.95)     범위: -0.95 ~ 0.95   기본값: 0.0   설명: 지연된 신호를 다시 입력에 섞는 비율입니다. 즉, 신호를 여러 번 순환시켜 더 진한 효과를 만듭니다.                  값       의미                       0.0       기본 플랜저 효과                 &gt; 0.0       점점 더 날카롭고 금속성 느낌                 &lt; 0.0       위상이 반전된 느낌, 다르게 움직이는 소리             4. Dry/Wet Mix (0~1)     범위: 0.0 ~ 1.0   기본값: 0.5   설명: 원래 소리(dry)와 플랜저 처리된 소리(wet)를 얼마나 섞을지 결정합니다.                  값       의미                       0.0       원본 소리만                 0.5       반반 혼합                 1.0       완전한 플랜저 효과만             🎧 예시 상황     🎸 일렉기타에 적용하면: “제트기 지나가는 듯한” 움직임이 생김   🧑‍🎤 보컬에 적용하면: 공중에 떠 있는 듯한 공간감   🥁 드럼에 적용하면: 리듬에 변화감 부여     📝 요약 표                 파라미터       설명       높일수록                       Frequency       LFO의 진동 속도       빠르게 움직이는 소리                 Depth       지연 시간의 변화 폭       강한 효과, 울렁거림                 Feedback       반복 횟수       금속적이고 진한 효과                 Dry/Wet       원본:이펙트 비율       100%면 완전히 변형된 소리             Flanger는 모듈레이션 기반 효과 중에서도 가장 개성 있는 이펙트 중 하나로, 잘만 쓰면 음원에 생동감과 움직임을 추가할 수 있습니다.  🎧 Flanger는 다음과 같은 상황에서 자주 사용됩니다. 핵심은 움직임·깊이감·특수 효과를 주고 싶을 때입니다:    🎸 1. 일렉트릭 기타     가장 전통적이고 널리 알려진 사용처입니다.   지연된 신호와 원음을 합쳐서 제트기 소리처럼 “쉭~ 우웅~” 하는 위상 간섭이 생김.   🎶 예: Van Halen – Ain’t Talkin’ ’Bout Love, The Police – Walking on the Moon      ✦ 특히 록, 메탈, 프로그레시브 록에서 배경을 몽환적으로 만들 때.     🎛️ 2. 신디사이저 / 패드 사운드     정적인 코드 진행에 움직임과 공간감을 부여함.   딜레이나 리버브처럼 공간계 효과는 아니지만, 모듈레이션 기반의 공간감을 줄 수 있음.   EDM, Ambient, Synthwave 등에서 많이 사용됨.     🥁 3. 드럼     하이햇이나 심벌, 때론 스네어에 플랜저를 걸어 리듬에 흐름감을 추가하거나,   빌드업 구간에서 점점 깊어지는 플랜저로 긴장감을 조성함.     🧑‍🎤 4. 보컬     리드보컬보다는 백킹보컬, 브리지 파트, 특수 효과용으로 사용   예를 들어, 꿈속 말하는 듯한 느낌이나 사이버 보이스를 연출할 때 적합     🧪 5. 효과음 (SFX)     영화/게임 등에서 레이저, 외계 소리, 사이버틱한 음향 효과를 만들 때 사용   일시적으로 강한 플랜저를 적용해 전환점, 강조 지점에 사용     ✅ 요약                 사용처       목적                       🎸 일렉기타       스윙감, 제트기 같은 소리                 🎛️ 신디사이저       패드에 깊이와 공간감 추가                 🥁 드럼       빌드업, 하이햇에 흘러가는 느낌                 🎤 보컬       몽환적, 미래적 분위기                 🧪 SFX       사이버틱, 외계적인 효과음             💡 팁: Flanger는 적게 쓰면 분위기 강화, 많이 쓰면 효과음화됩니다. 컨텍스트에 맞게 조절하는 것이 중요합니다. "
  },
  
  {
    "title": "AudioKit의 Expander",
    "url": "/posts/Expander/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-05 14:05:42 +0900",
    "content": "Expander  Expander는 작은 소리를 더 작게 만들어서 잡음을 줄이거나, 다이내믹 레인지를 넓히는 데 사용되는 오디오 효과입니다. 압축기(Compressor)와 반대 개념입니다. 아래는 각 파라미터의 역할과 수치 변화에 따른 음향적 효과를 설명한 내용입니다.    🔧 Expander 파라미터 상세 설명  1. Expansion Ratio (확장 비율)     설명: Threshold 이하의 소리 신호를 얼마나 줄일지를 결정합니다.   값 범위: 1.0 ~ 50.0   기본값: 2.0        작동 방식:             비율이 1.0:1이면 확장 없음 (flat)       비율이 2.0:1이면 threshold보다 2dB 낮은 입력은 4dB 감소       비율이 높을수록 작은 소리를 더 작게 만들어, 배경 소음 제거 효과 증가           ⬆ 증가 시 효과:     더 과감한 **노이즈 게이팅(noise gating)**처럼 작동   소리가 작을 때 거의 무음에 가깝게 됨   ⬇ 감소 시 효과:     소리의 자연스러움을 유지하며 미세한 잡음만 억제     2. Expansion Threshold (확장 임계값)     설명: 확장이 적용되기 시작하는 입력 레벨 (dBFS)   값 범위: -120.0 ~ 0.0 dB   기본값: 0.0   ⬆ 증가 시 효과:     더 큰 소리에만 확장이 적용되어, 덜 민감하게 작동   ⬇ 감소 시 효과:     아주 작은 신호도 확장되어, 미세한 소리까지 억제됨     3. Attack Time (공격 시간)     설명: 신호가 threshold를 넘어섰을 때 확장이 해제되는 시간 (초)   값 범위: 0.001 ~ 0.3 초   기본값: 0.001   ⬆ 증가 시 효과:     느리게 복원되므로 작아진 신호가 천천히 커짐 → 부드러운 동작   ⬇ 감소 시 효과:     빠르게 반응하여 더 뚜렷하게 변화 → 팍팍 꺼짐 느낌     4. Release Time (릴리스 시간)     설명: 신호가 다시 threshold 아래로 떨어졌을 때 확장이 다시 적용되기까지 걸리는 시간   값 범위: 0.01 ~ 3.0 초   기본값: 0.05   ⬆ 증가 시 효과:     확장 효과가 천천히 적용됨, 더 자연스럽게 소리 줄어듦   ⬇ 감소 시 효과:     빠르게 확장 적용, 즉시 작게 만듦 → 노이즈 컷에 유리하지만 부자연스러울 수 있음     5. Master Gain (출력 볼륨 보정)     설명: 전체 출력 신호의 최종 볼륨 조절   값 범위: -40.0 ~ +40.0 dB   기본값: 0.0   ⬆ 증가 시 효과:     전체 출력 신호의 음량을 올림   ⬇ 감소 시 효과:     전체 음량을 줄여서 더 부드러운 소리로 만듦     📝 요약                 파라미터       설명       증가 시 효과                       Expansion Ratio       작은 소리의 감소 비율       배경 소음 강하게 억제                 Threshold       확장 시작 레벨       확장 작동 범위 축소                 Attack Time       확장 해제 반응 속도       부드럽고 느린 복원                 Release Time       확장 적용 반응 속도       자연스러운 볼륨 감소                 Master Gain       전체 출력 볼륨       최종 볼륨 보정             🎧 응용 예시     보컬 트랙에서 리버브나 호흡 소리를 줄이고 싶을 때   녹음된 소스의 잡음을 제거할 때   부드러운 게이트처럼 사용하여 공간감을 조절할 때   Expander는 Compressor보다 쓰임새가 정교하지만, 잘 조절하면 소리의 명료도와 깨끗함을 향상시킬 수 있습니다. "
  },
  
  {
    "title": "음향: 딜레이(Delay)란?",
    "url": "/posts/Delay/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-05 14:03:00 +0900",
    "content": "Delay  음향에서 딜레이(Delay)는 소리를 일정 시간 지연시켜 다시 재생하는 시간 기반 이펙트(time-based effect)입니다. 이는 원래의 소리(dry signal)에 지연된 복제본(wet signal)을 더하여 공간감, 깊이, 리듬감, 반복 효과 등을 만들어냅니다.    딜레이의 기본 개념     입력된 소리를 n 밀리초(또는 초) 동안 기다렸다가 다시 재생   인간의 귀는 30ms 이상의 지연을 “반사” 또는 “메아리”처럼 인식함     예시          에코(Echo)             “하이”라고 말하면, 잠시 후 “하이… 하이…”가 반복됨 → 딜레이 타임과 피드백이 큰 경우                슬랩백 딜레이(Slapback Delay)             짧은 딜레이(약 80~120ms)로 “하이-하이” 느낌 → 일렉트릭 기타 등에서 흔히 사용됨             주요 매개변수                 매개변수       설명                       Delay Time       소리를 얼마나 늦게 재생할지 (ms 또는 s)                 Feedback       지연된 소리를 몇 번 반복할지 (0.0 ~ 1.0)                 Mix / Balance       원래 소리와 지연된 소리의 비율 조절                 Wet/Dry       Wet = 효과음, Dry = 원래 소리             딜레이의 응용                 응용 분야       설명                       보컬 딜레이       라이브 공연, 믹싱에서 공간감과 감정을 강조                 기타 딜레이       음을 겹쳐 풍부한 사운드를 만듦                 EDM / 힙합       리듬을 강조하고 루프 효과를 추가                 앰비언트 사운드       깊고 지속적인 공간감 생성             작동 원리 (기초적인 알고리즘)  [Input Signal] → [Delay Buffer] → [Output]                               ↑                        (Feedback Loop)      소리가 Delay Buffer에 저장됨   정해진 시간이 지나면 출력으로 나감   Feedback이 있으면 그 출력이 다시 Delay Buffer로 들어가 재반복됨     딜레이 vs 리버브                 항목       딜레이       리버브                       지연 횟수       명확한 반복       수많은 잔향                 공간 표현       메아리, 반복       방 크기, 반사                 시간 간격       수십 ms ~ 수 초       보통 짧은 밀리초 단위 다수                 용도       리듬, 강조       공간감, 자연스러움             요약     딜레이는 소리를 늦게 재생하여 반복, 에코, 리듬, 공간 효과를 만듦   다양한 음악 장르와 사운드 디자인에서 핵심적인 역할   딜레이 시간, 피드백, 믹스 값에 따라 느낌이 크게 달라짐   따라서 딜레이는 단순히 소리를 반복하는 것을 넘어 감정과 공간을 디자인하는 도구라고 할 수 있습니다.  "
  },
  
  {
    "title": "음향: 데시벨(db)이란?",
    "url": "/posts/Decibel/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-05 14:03:00 +0900",
    "content": "Decibel    🎧 1. dB란 무엇인가요?  ▸ “데시벨”은 소리의 크기를 표현하는 단위입니다.     원래 이름은 “벨(Bell)”이었고, 전화기의 발명자 알렉산더 그레이엄 벨의 이름에서 따왔습니다.   하지만 벨은 너무 큰 단위여서, 실제로는 **1/10 크기인 “데시벨(dB)”**을 사용합니다. → 즉, 1 벨 = 10 dB   ✅ 왜 소리를 숫자로 표현해야 하나요?     우리의 귀는 아주 작은 소리부터 굉장히 큰 소리까지 엄청나게 넓은 범위를 들을 수 있습니다.   그런데 이걸 그냥 1, 2, 3으로 표현하면, 너무 급격하게 늘어나거나 너무 작게 보여서 측정하기도, 비교하기도 불편합니다.     🔢 2. 왜 로그(log) 단위를 쓸까요?  ▸ 사람은 소리의 “비율”을 인식합니다.     예: ● 1에서 2로 커졌을 때 → “어, 좀 더 크네?” ● 10에서 20으로 커졌을 때 → “어, 또 좀 더 크네?” ▶ 우리는 2배 차이일 때 비슷한 느낌을 받습니다. → 이것이 로그 감각입니다.   ✅ 로그는 어떻게 적용되나요?     예: 소리의 에너지가 10배로 커지면 → +10 dB 에너지가 2배로 커지면 → +3 dB (정확히는 약 3.01 dB) 에너지가 절반이 되면 → -3 dB   🎯 즉, dB는 소리의 절대적인 크기보다, 변화의 ‘배율’을 표현하는 단위입니다.    📏 3. 상대값 vs 절대값 dB  ✅ dB (상대값)     기준이 없음, 단순히 얼마나 커졌는지/작아졌는지만 말합니다.   “이 소리, 6dB 더 키워줘” → 지금 상태보다 6dB만큼 증가시키라는 말.   실무에서 믹싱할 때 가장 많이 쓰는 표현입니다.   ✅ dBu, dBV, dB SPL, dBFS 등 (절대값)     기준이 있는 dB입니다. 즉, “얼마나”가 아니라 “몇 볼트인지, 몇 데시벨인지”를 정확히 수치로 표현한 것.                  단위       기준값       용도 예시                       dBu       0.775V RMS       프로용 오디오 장비                 dBV       1V RMS       가정용 오디오 장비                 dBm       1mW (600Ω 기준)       구형 전화/오디오 장비                 dB SPL       20μPa (마이크 기준 압력)       소리의 실제 크기 (데시벨 측정기)                 dBFS       디지털 최대값 (0dBFS = 클리핑)       디지털 오디오 믹싱/마스터링             🎛️ 4. VU 미터 vs 실제 전압  ▸ 0VU는 진짜 ‘0’이 아닙니다!     오디오 장비에는 흔히 VU 미터라는 눈금계가 있습니다.   이 눈금의 0점은 장비마다 기준이 다릅니다:                  장비 종류       0VU 기준       실제 단위       전압(V)                       프로용       +4dBu       약 1.23V       고급 장비 (스튜디오 등)                 아마추어용       -10dBV       약 0.316V       가정용 오디오, 캠코더 등           🎯 즉, 같은 0VU라고 해도, 어떤 장비에선 더 크고, 어떤 장비에선 더 작게 들릴 수 있습니다. 이게 인터페이스 매칭이 중요한 이유입니다.    💡 5. dB를 쉽게 이해하는 비유                 개념       설명                       dB (상대값)       “앞으로 3칸만 가!” → 지금 위치에서의 변화량                 dBu, dBV 등 (절대값)       “지금 너는 정확히 1.23V 위치야” → 정확한 수치 기준             ✅ 최종 요약     dB: 변화량 (상대 단위) → “지금보다 6dB 키워줘”   dBu, dBV: 정확한 기준이 있는 수치 → 장비 연결 시 중요   dB SPL: 환경 소음 등 실제 소리의 크기 측정 시 사용   dBFS: 디지털 오디오에서 소리 크기의 최댓값 기준 (0이 최고치, 더 크면 클리핑)   VU 미터: 장비에 따라 기준이 다르므로 주의 필요     수치 및 변환 예제  아래는 dBu, dBV, dBFS, dB SPL 등의 대표적인 dB 단위들의 수치 예제와 상호 변환 예제를 상세히 정리한 내용입니다. 수식도 함께 소개하겠습니다.    📐 1. dBu, dBV, dBFS, dB SPL 간단 요약                 단위       기준 값       의미       주로 쓰이는 분야                       dBu       0.775V (RMS)       전압 기준, 프로 오디오 장비용       믹서, 인터페이스 등                 dBV       1V (RMS)       전압 기준, 가정용/소비자 오디오용       CDP, 하이파이 등                 dBFS       디지털 최대값 (0dBFS)       디지털의 최대값 기준       DAW, 디지털 믹싱                 dB SPL       20μPa       소리의 압력 기준       마이크, 환경 소음계             🧮 2. dBu ↔ 전압 변환 예제  공식:     V (RMS) = 0.775 × 10^(dBu / 20)   dBu = 20 × log10(V / 0.775)                  dBu       전압 (V RMS)       비고                       0 dBu       0.775V       기준값                 +4 dBu       약 1.23V       프로 오디오 표준                 -10 dBu       약 0.245V       약한 신호 수준                 +10 dBu       약 2.45V       비교적 강한 신호             🧮 3. dBV ↔ 전압 변환 예제  공식:     V (RMS) = 10^(dBV / 20)   dBV = 20 × log10(V)                  dBV       전압 (V RMS)       비고                       0 dBV       1.00V       기준값                 -10 dBV       약 0.316V       컨슈머 오디오 표준                 +6 dBV       약 2.00V       강한 신호 수준             🔄 4. dBu ↔ dBV 변환 예제  공식:     dBV = dBu - 2.21   dBu = dBV + 2.21                  dBu       dBV       설명                       +4 dBu       +1.79 dBV       프로 오디오 기준 전환값                 0 dBu       -2.21 dBV                         -10 dBV       -7.79 dBu                     🧮 5. dBFS 예제 (디지털 오디오)     dBFS는 **디지털 오디오에서 최대치(클리핑 지점)**를 0으로 놓고 상대값을 계산합니다.   일반적으로 0 dBFS가 가장 큰 소리, 그보다 작으면 음량이 낮은 상태입니다.                  dBFS       의미                       0 dBFS       디지털 최대값 (클리핑 지점)                 -6 dBFS       최대값의 약 50%                 -12 dBFS       최대값의 약 25%                 -18 dBFS       일반적인 믹싱 기준 (헤드룸 확보)                 -60 dBFS       아주 약한 신호           🎯 dBFS는 절대값처럼 보이지만 디지털 장비 기준의 상대값이라고 이해하셔도 좋습니다.    🧮 6. dB SPL 예제 (음압)  공식:     dB SPL = 20 × log10(P / Pref) 여기서 Pref = 20μPa (2 × 10^-5 Pa) = 인간이 들을 수 있는 가장 작은 소리                  dB SPL       실생활 예시                       0 dB SPL       거의 무음 (사람이 간신히 들을 수 있는 소리)                 30 dB SPL       속삭임, 아주 조용한 방                 60 dB SPL       일반 대화                 85 dB SPL       도로 소음, 위험 경계선                 100 dB SPL       지하철, 시끄러운 음악                 120 dB SPL       귀가 아픈 소리, 통증 시작 지점             🔁 예제 변환 시나리오  🎧 예제 1: +4 dBu는 몇 dBV인가요?     dBV = dBu - 2.21 = 4 - 2.21 = 1.79 dBV     🎧 예제 2: 0.316V는 몇 dBu인가요?     dBu = 20 × log10(0.316 / 0.775)   계산하면 ≈ -10.2 dBu     🎧 예제 3: -6 dBFS는 dBu로 얼마일까요?     이건 디지털 장비가 아날로그로 변환되는 출력 단계에서만 의미가 있습니다.   예: 디지털 인터페이스의 기준 출력이 0 dBFS = +18 dBu라고 가정하면, → -6 dBFS는 약 +12 dBu에 해당합니다. (이는 장비 설정에 따라 다릅니다)     🧭 참고 팁     dBu, dBV는 전압 기반   dBm은 전력 기반 (잘 안 쓰임)   dBFS는 디지털 최대값 기준   dB SPL은 공기의 압력 기반     "
  },
  
  {
    "title": "AudioKit의 DynamicRangeCompressor",
    "url": "/posts/DynamicRangeCompressor/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-03 14:11:05 +0900",
    "content": "DynamicRangeCompressor     코드 보기   DynamicRangeCompressor는 오디오의 다이내믹 레인지(Dynamic Range), 즉 가장 조용한 소리와 가장 큰 소리 사이의 차이를 줄이는 데 사용되는 오디오 이펙트입니다. 이 컴프레서는 작은 소리는 상대적으로 키우고, 큰 소리는 줄여서 전체 소리를 더 일관되고 듣기 쉽게 만듭니다.    역할: DynamicRangeCompressor란?     소리가 너무 크면 자동으로 줄여주고, 너무 작으면 상대적으로 키워줌   방송, 팟캐스트, 보컬 믹싱, 마스터링 등 다양한 분야에서 사용   AudioKit의 DynamicRangeCompressor는 실시간으로 이 처리를 수행     주요 파라미터 설명                 파라미터 이름       기본값       범위       의미                       ratio       1.0       0.01 ... 100.0       압축 비율 (압축 강도)                 threshold       0.0       -100.0 ... 0.0 dB       압축이 시작되는 기준 레벨                 attackDuration       0.1       0.0 ... 1.0 sec       소리가 커졌을 때 얼마나 빠르게 압축을 시작할지                 releaseDuration       0.1       0.0 ... 1.0 sec       소리가 다시 작아졌을 때 얼마나 천천히 압축을 풀지             파라미터 상세 설명  1. Ratio (비율)     압축 비율: 입력이 threshold를 넘었을 때 얼마나 줄일지   예: ratio = 4.0 → threshold보다 4dB 큰 입력이 들어오면 출력은 1dB만 증가 (4:1)            참고: 1(1:1)은 압축 없음, 100.0(∞:1)은 리미터처럼 작동           값이 클수록 압축이 강해짐                  Ratio       효과                       1.0       압축 없음                 2.0       부드러운 압축                 10.0 이상       리미터에 가까운 강한 압축             2. Threshold (임계값)     몇 dB 이상부터 압축을 적용할지 설정   예: threshold = -20.0 → 입력 레벨이 -20dB보다 크면 압축 시작   일반적으로 보컬: -12dB ~ -24dB 권장     3. Attack Duration (공격 시간)     압축 시작까지의 시간 지연   짧으면 소리의 순간적인 피크도 줄이고, 길면 어택(강조되는 처음 부분)을 살림                  값       설명                       0.01~0.05초       매우 빠름 (피크 억제)                 0.1초 이상       느림 (자연스러움)             4. Release Duration (해제 시간)     입력이 threshold 아래로 떨어졌을 때, 압축을 완전히 해제하기까지의 시간   짧으면 빠르게 원래대로, 길면 부드러운 복원     사용 예시          보컬 다이내믹 정리:             threshold = -20, ratio = 3, attack = 0.05, release = 0.3                마스터링 전체 트랙:             threshold = -10, ratio = 1.5, attack = 0.1, release = 0.5             요약                 파라미터       설명                       ratio       출력 레벨을 얼마나 압축할지                 threshold       압축을 시작할 기준 입력 레벨                 attackDuration       압축 시작까지의 시간                 releaseDuration       압축 해제까지 걸리는 시간           DynamicRangeCompressor는 음원의 불균형을 다듬고, 더 프로페셔널한 사운드를 만들어주는 핵심 도구입니다. AudioKit에서는 이 파라미터들을 실시간으로 조절하여 효과를 직관적으로 확인할 수 있습니다. "
  },
  
  {
    "title": "음향에서 Low Pass Cutoff 란?",
    "url": "/posts/LowPassCutoff/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-02 14:36:36 +0900",
    "content": "Low Pass Cutoff  Low Pass Cutoff는 오디오 필터에서 저역 통과 필터(Low-Pass Filter, LPF)의 핵심 설정값입니다. 아래에서 LPF의 개념과 cutoff frequency가 실제로 어떤 영향을 주는지 자세히 설명드리겠습니다.    Low-Pass Filter (저역 통과 필터)란?     저주파수(낮은 소리)는 통과시키고, 고주파수(높은 소리)는 차단하는 필터      주로 소리를 부드럽게 만들거나, 고주파 잡음을 줄일 때 사용   Delay, Reverb, Distortion 등 다양한 효과에서 필터링 용도로 포함됨     Cutoff Frequency (컷오프 주파수)     “어디까지를 통과시키고, 그 이상부터는 감쇄시킬지”를 결정하는 경계점   예시:     cutoff = 15000Hz → 15kHz 이하의 소리는 그대로 통과, 15kHz 이상은 점점 약해지며 감소   감쇄는 점진적으로 일어남  → 흔히 -6dB/octave, -12dB/octave 등의 기울기(slope)를 가짐 즉, 컷오프를 지난 주파수는 갑자기 잘리는 게 아니라, 부드럽게 약해짐    시각적 비유 (간단한 ASCII 그래프)  출력 레벨 100% |        _______      |       /      |      /      |     /      |    /      |   /    ← 점점 줄어드는 고주파수      |__/         ↑      컷오프 주파수     AudioKit의 Delay에서의 역할  AudioKit의 Delay 노드는 내부적으로 딜레이된 신호가 점점 줄어들며 반복되는데, 여기서 고주파 성분이 계속 남아 있으면 날카로운 잔향이 생깁니다.  따라서:     cutoff = 15000Hz → 대부분의 고역은 유지 (선명한 딜레이)   cutoff = 4000Hz → 고역을 많이 줄이고, 더 부드러운 느낌의 딜레이로 바뀜   cutoff = 1000Hz 이하 → 딜레이가 저음 위주로만 들림 (무겁고 둔탁함)     요약                 용어       설명                       Low-Pass Filter       고주파를 줄이고 저주파는 통과시키는 필터                 Cutoff Frequency       고주파 차단이 시작되는 지점 (Hz)                 AudioKit 활용       딜레이 또는 리버브에서 잔향을 부드럽게 하거나, 고주파수를 줄이기 위해 사용             실용 팁:     보컬 딜레이 → cutoff ≈ 8000~12000Hz : 선명하면서 자연스러움   기타 앰비언트 → cutoff ≈ 4000~6000Hz : 부드럽고 깊이감 있음   베이스 → cutoff ≈ 1000Hz 이하 : 저음의 웅장함 유지   필요에 따라 cutoff frequency를 조절하여, 딜레이 톤을 “밝게” 혹은 “어둡게” 만드는 도구로 생각하시면 됩니다. "
  },
  
  {
    "title": "AudioKit의 Delay 및 DryWetMixer",
    "url": "/posts/Delay+DryWetMixer/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-02 14:32:57 +0900",
    "content": "Delay  DelayConductor 클래스는 오디오 입력을 받아 지연 효과(Delay)를 적용한 후, 원본 신호(dry)와 효과가 적용된 신호(wet)를 DryWetMixer로 섞어서 출력하는 구조를 갖습니다. 아래는 코드와 함께 각 구성 요소 및 주석의 의미를 상세히 설명한 내용입니다.    🎛 클래스 역할 및 구조  DelayConductor는 다음을 수행합니다:     기본 오디오 소스(.strings)를 메모리에 로드하고   AudioPlayer로 재생   Delay 이펙트를 걸고   원본과 이펙트가 적용된 신호를 DryWetMixer로 섞어 출력     🧱 주요 구성 해설  🔹 delay = Delay(player)     지연 효과를 적용할 노드입니다.   원본 player로부터 오디오를 입력 받아서 처리합니다.   🔹 delay.feedback = 0.9     피드백(Feedback): 효과된 신호가 다시 입력으로 되돌아가 반복되는 비율입니다.   0.9이면 한 번의 딜레이로 끝나지 않고, 서서히 줄어드는 에코처럼 들림.   🔹 delay.time = 0.01     지연 시간 (단위: 초)   0.01초 → 빠르게 반복되는 딜레이 → 거의 플랜저(flanger)나 코러스 같은 효과로 들릴 수 있음.     🧪 delay.dryWetMix = 100의 의미  // 지연에 내장된 dry/wet 믹스를 사용하지 않습니다. // dry/wet 결과를 탭(tapping)하여 그래프로 표시하기 때문입니다. delay.dryWetMix = 100      기본적으로 Delay 노드에도 자체적인 Dry/Wet Mix가 있지만,        그래프 표시를 위해 효과된 신호만 분리해서 사용하고 싶을 경우, dryWetMix = 100으로 설정합니다.             즉, Delay 노드가 순수한 Wet 신호만 출력하도록 함           대신에 외부의 DryWetMixer로 dry/wet 비율을 통합 조절합니다.     🧩 dryWetMixer = DryWetMixer(player, delay)     💡 주석 해석: “두 개의 입력을 정확히 믹싱하는 것은 매우 흔한 일입니다. 하나는 처리 전, 다른 하나는 처리 후, 그 결과 두 입력이 결합된 결과가 생성됩니다.”      DryWetMixer는 player(dry 신호)와 delay(wet 신호)를 합칩니다.   이런 방식은 오디오 시각화(UI 그래프) 또는 커스텀 효과 체인 구성에 더 유연함을 줍니다.     📊 Delay 파라미터 정리                 파라미터       기본값       범위       설명                       Dry-Wet Mix       100.0       0.0~100.0       Delay 자체의 내부 믹스 → 이 코드에서는 사용 안함                 Delay Time       0.01초       0.0001~2.0       얼마나 지연되었는지, 0.01초는 매우 짧은 딜레이                 Feedback       0.9       -99.9~99.9       에코의 지속 시간에 영향, 0.9는 반복이 길고 강함                 Low Pass Cutoff       15000Hz       10~22050       딜레이 신호의 고역 제한 (고주파수 컷오프)             🧠 요약     DelayConductor는 원본 소리와 효과 적용된 소리를 DryWetMixer로 분리해서 다룸   내장 믹스를 사용하지 않고 외부에서 직접 믹싱 → 시각화나 제어에 유리   DryWetMixer는 특히 복잡한 효과 체인이나 직관적 제어 UI 구성에 적합함     이 방식은 향후 다른 이펙트 체인(예: 리버브 → 컴프레서 → 코러스 등)을 구성할 때도 효과적으로 확장 가능합니다. "
  },
  
  {
    "title": "AudioKit의 Convolution",
    "url": "/posts/Convolution/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-06-01 14:37:39 +0900",
    "content": "Convolution  이 코드는 AudioKit과 SoundpipeAudioKit을 사용해 컨볼루션 리버브(Convolution Reverb) 효과를 구현한 예제입니다. 특히, 두 개의 다른 임펄스 응답(Impulse Response) 파일을 혼합하여 사용자 지정 리버브 환경을 만드는 것이 핵심입니다.    🔊 핵심 개념 요약  🔹 Convolution     Convolution Reverb는 실제 공간에서 녹음된 ‘임펄스 응답(Impulse Response)’과 입력 오디오 신호를 **곱셈-합산 연산(convolution)**하여, **실제 공간의 리버브를 시뮬레이션합니다.   입력 오디오와 IR 파일을 합성(convolve)하여 현실적인 반향 효과를 구현합니다.     📌 간단 정리                 용어       설명                       Convolution       입력 신호와 임펄스 응답(IR)을 수학적으로 합성하여 실제 공간과 유사한 반향을 만드는 과정                 Impulse Response (IR)       특정 공간에서 “짧고 강한 소리”(예: 박수)를 녹음한 오디오 파일. 공간의 잔향 특성을 포함                 Ftable (Function Table)       AudioKit과 같은 DSP 환경에서 사용하는 신호 데이터 테이블, 예: 샘플/파형/IR 등을 담은 배열             🎛️ Convolution 클래스 정의  // This module will perform partitioned convolution on an input signal using an ftable as an impulse response.      input signal: 마이크, 음원 등의 오디오 신호   ftable: 임펄스 응답 데이터를 담고 있는 파형 테이블, 흔히 .wav 파일에서 변환됨   partitioned convolution: IR을 여러 작은 조각으로 나눠 계산해 CPU 부하를 줄이고 지연(latency)을 줄이는 방식     🔉 예시로 이해하기     당신이 플루트 소리를 건물 계단실에서 연주했다고 가정   실제 계단실에서 녹음된 IR (Impulse Response) 파일: stairwell.wav        AudioKit의 Convolution은:             입력된 플루트 소리를 stairwell.wav의 ftable과 합성해, 마치 그 계단실에서 직접 플루트를 연주한 것처럼 들리게 함             💡 왜 ftable을 쓰는가?     오디오를 실시간 처리하기 위해선 미리 처리된 IR 데이터를 배열(테이블)로 변환하는 게 효율적   이 테이블을 기반으로 빠르게 수학적 곱셈/합산 연산을 진행할 수 있음     🧱 구조 및 동작 흐름  1. 🔧 ConvolutionData  struct ConvolutionData {   var dryWetMix: AUValue = 0.5   var stairwellDishMix: AUValue = 0.5 }      dryWetMix: 원본 소리(player)와 리버브 처리된 소리(믹서 출력) 간 비율.   stairwellDishMix: 두 리버브 (stairwell, dish) 간 비율.     2. 🛠️ 임펄스 응답 파일 불러오기  let stairwellURL = ... let dishURL = ...           각각의 리버브 환경을 묘사하는 .wav 파일             stairwell.wav: 계단실과 같은 큰 공간 느낌       dish.wav: 금속성 또는 특이한 잔향 느낌             3. 🔄 Convolution 효과 구성  stairwellConvolution = Convolution(player, impulseResponseFileURL: stairwellURL, partitionLength: 8192) dishConvolution = Convolution(player, impulseResponseFileURL: dishURL, partitionLength: 8192)      partitionLength: 처리 블록 크기. 값이 작을수록 지연은 줄지만 CPU 사용률이 증가   두 개의 서로 다른 공간 리버브 효과 생성     4. 🔀 DryWetMixer 체인  stairwellDishMixer = DryWetMixer(stairwellConvolution, dishConvolution, balance: 0.5) dryWetMixer = DryWetMixer(player, stairwellDishMixer, balance: 0.5) engine.output = dryWetMixer      리버브 A와 B를 섞은 결과 → 원본과 또 섞음   최종 출력은 dryWetMixer        [Player]         │  ┌─────┴─────┐  ▼           ▼ Stairwell   Dish   │           │   └─────┬─────┘         ▼  stairwellDishMixer         │         ▼    dryWetMixer (← player)         │      [Engine Output]     7. 🧪 리버브 활성화  stairwellConvolution.start() dishConvolution.start()      AudioKit의 Node는 사용 전 .start() 필수     🧩 요약                 요소       설명                       player       오디오 재생용                 Convolution       IR 기반 리버브 처리                 stairwellDishMixer       두 리버브를 섞는 믹서                 dryWetMixer       원본과 리버브를 섞는 믹서                 data       두 믹서의 balance를 실시간 제어             📌 활용 예시     게임에서 공간 이동에 따른 잔향 효과   음악에서 마이크 녹음 시 실제 공간 질감 추가   Foley 사운드 제작    "
  },
  
  {
    "title": "AudioKit에서 AudioPlayer 재생 길이 문제",
    "url": "/posts/AudioPlayer%EC%9E%AC%EC%83%9D%EA%B8%B8%EC%9D%B4%EB%AC%B8%EC%A0%9C/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-31 16:02:15 +0900",
    "content": "AudioPlayer 재생 길이 문제  Q. 코드에서 음원 파일을 로딩해서 재생하는 기능이 있는데 재생 길이가 들쭉날쭉한데 (어쩔 떄는 전체 재생하는데, 어쩔 떄는 중간에 끊김) 해결방법은? A. Buffer 문제 가능성일 수 있음 ❌ Q.conductor.player.buffer 를 nil 로 하더라도 동일한 문제가 발생하는데 다른 곳에 문제는 없음?   맞는 답변  conductor.player.buffer = nil로 설정해도 문제가 동일하게 발생한다면, 문제는 버퍼 자체보다는 player.file과 player.buffer의 동시 사용 방식 또는 AVAudioEngine의 연결 구조에 있을 가능성이 높습니다. AudioPlayer는 buffer 또는 file 중 하나만 사용할 수 있으며, 둘을 동시에 설정하면 예기치 못한 동작(중간에 끊김 등)이 발생할 수 있습니다.    ✅ AudioKit AudioPlayer 동작 방식 요약                 설정된 값       동작 방식                       .file만 설정       스트리밍 재생 (긴 오디오에 적합)                 .buffer만 설정       메모리에 올려서 재생 (짧은 클립에 적합)                 둘 다 설정       ⚠️ 비권장 — 예기치 않은 문제 발생 가능             🔍 지금 코드의 의심 지점  conductor.player.file = try? AVAudioFile(forReading: url) conductor.player.isLooping = true conductor.player.buffer = buffer   위처럼 file과 buffer를 동시에 설정하면 충돌 가능성이 있습니다.  추가  Effects 코드의 오디오 플레이어 설정 방법  버퍼만 설정  buffer = Cookbook.sourceBuffer(source: defaultSource) player.buffer = buffer player.isLooping = true   기존 PlayersControl 코드  버퍼 + 오디오 파일 설정이 추가?  conductor.player.file = try? AVAudioFile(forReading: url) // &lt;- ?? conductor.player.isLooping = true conductor.player.buffer = buffer     ✅ 해결 방법  방법 1: **file을 지우고 buffer만 사용 ☑️  **  conductor.player.stop() conductor.player.file = nil // file은 사용하지 않음 conductor.player.buffer = buffer conductor.player.isLooping = true   방법 2: **file만 사용하고 buffer는 설정하지 않음 ❌ **  conductor.player.stop() conductor.player.buffer = nil // buffer는 사용하지 않음 conductor.player.file = try? AVAudioFile(forReading: url) conductor.player.isLooping = true   둘 중 하나만 사용하세요.    📌 추가 확인 사항     engine.output이 올바르게 연결되어 있는지 (AudioPlayer가 output chain에 포함되어야 함)   buffer.frameLength가 실제 음원 길이와 일치하는지 (짧게 잘려 있지 않은지)   .play() 이후에 isPlaying이 true인지 확인 (player.isPlaying 체크)     🧪 디버깅 팁  Log(\"buffer length:\", buffer.frameLength) Log(\"buffer duration:\", buffer.frameLength / buffer.format.sampleRate)   또는  Log(\"file duration:\", file.length / file.processingFormat.sampleRate)   을 통해 실제 재생되는 길이를 확인하세요.    ✅ 결론  buffer와 file은 동시에 쓰면 안 됩니다. 둘 중 하나만 설정해야 하고, 중간에 끊김 문제가 있다면 이중 설정을 제거하는 것부터 확인해야 합니다. "
  },
  
  {
    "title": "음향: 컴프레서(Compressor)란 무엇인가?",
    "url": "/posts/%EC%BB%B4%ED%94%84%EB%A0%88%EC%84%9C(Compressor)/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-31 15:19:22 +0900",
    "content": "컴프레서 (Compressor)  음향에서 **Compressor(컴프레서)**는 오디오 신호의 다이내믹 레인지(Dynamic Range) — 즉, 가장 작은 소리와 가장 큰 소리 사이의 음량 차이 — 를 줄이는 도구입니다.    ✅ “소리를 압축한다”는 의미  “압축”이란 말 그대로 소리의 볼륨 차이를 줄이는 것입니다. 즉, 너무 큰 소리는 줄이고, 작은 소리는 상대적으로 부각시켜서 전체적으로 보다 균형 잡힌 소리를 만듭니다.  예를 들어, 녹음된 보컬에서 어떤 단어는 너무 크고 어떤 단어는 너무 작게 들릴 수 있는데, 컴프레서는 이 차이를 줄여 일정한 볼륨감으로 만들어줍니다.    🎯 컴프레서를 사용하는 이유  1. 볼륨 균형 맞추기     마이크 앞에서 말하는 사람이 갑자기 크게 말하거나, 작게 말하면   컴프레서가 이를 감지해 큰 소리는 줄이고, 작은 소리는 상대적으로 부각 → 듣는 사람이 더 편안하게 들을 수 있게 합니다.   2. 사운드를 더 밀도 있게 만들기     다양한 악기의 음량 차이를 줄이면 믹스 전체가 더 탄탄하고 밀도 있게 들립니다.   특히 방송, 팟캐스트, 광고 등에서는 매우 중요   3. 음악적인 효과     특정 사운드에 컴프레서를 걸면 소리가 더 세게, 더 타이트하게 느껴짐   드럼이나 베이스에 컴프레서를 쓰면 퍼지는 느낌을 잡아주고, 명확한 어택감을 줌     📊 예시로 보는 작동 방식  예를 들어,     Threshold(문턱값)를 -20dB로 설정해 놓으면   입력되는 소리 중 -20dB보다 큰 신호만 컴프레서가 반응합니다.   그리고 그 큰 신호를 미리 정한 비율(Ratio)만큼 줄입니다.                  입력 레벨       출력 레벨 (예시)                       -10 dB (크다)       -15 dB (줄어듦)                 -30 dB (작다)       -30 dB (그대로)           결과적으로 **소리의 볼륨 차이(Dynamic Range)**가 줄어듭니다.    📍 어떤 상황에서 주로 사용?                 사용 대상       사용 목적                       보컬       단어별 음량 차이 정리                 드럼       피크 컨트롤, 명확한 어택                 기타       코드 연주 시 일관성 유지                 전체 믹스 (마스터링)       전체 곡의 일관된 출력 볼륨 확보             📌 마무리  컴프레서는 음악 제작, 음향 편집, 방송, 게임 사운드 디자인 등 거의 모든 오디오 작업에 필수적입니다. 적절히 사용하면 듣기 좋은, 깔끔한 소리를 만들 수 있고 과하게 사용하면 소리가 답답하거나 인위적이 될 수 있으니 주의가 필요합니다. "
  },
  
  {
    "title": "음향에서 오실레이터(Oscillator)",
    "url": "/posts/%EC%98%A4%EC%8B%A4%EB%A0%88%EC%9D%B4%ED%84%B0(Oscillator)/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-29 13:08:10 +0900",
    "content": "오실레이터 (Oscillator)    오실레이터(oscillator)는 지속적인 주기적 파형을 생성하는 소리의 원천입니다. 디지털 오디오나 신시사이저에서 기본적인 “소리 생성 장치”라고 생각하시면 됩니다.    오실레이터란?          정의: 오실레이터는 사인파, 사각파, 톱니파 등과 같은 주기적인 파형을 생성하는 신호 발생기입니다.           역할: 디지털 음악에서 주파수(frequency)와 진폭(amplitude)를 설정하면, 해당 파형의 음을 지속적으로 만들어냅니다. → 즉, 음고(높이)와 볼륨을 결정하는 신호를 생성        예: 사인파 오실레이터  let osc = Oscillator() osc.frequency = 440  // A4 음 (라) osc.amplitude = 0.5 osc.start()      440Hz → 초당 440번 진동하는 사인파 → 우리가 듣기에 “라”로 인식되는 소리   start()를 호출하면 파형이 재생되기 시작함     자주 사용하는 파형 종류                 파형       설명       소리 느낌 예시                       Sine wave       가장 부드러운 파형 (기본음)       튜닝 포크, 플루트                 Square wave       짧고 강한 진동 (홀수 배수 포함)       8비트 게임 사운드                 Sawtooth       치솟았다가 뚝 떨어지는 모양       브라스, 신시 리드음                 Triangle       부드럽지만 약간 날카로움       클라리넷 느낌             요약                 항목       설명                       기능       음파 생성기                 제어 요소       frequency, amplitude, waveform 등                 사용 예시       신시사이저 기본 음, 튜너, 테스트 톤, 효과음 생성 등             즉, 오실레이터는 “음향의 첫 단추”로, 소리를 만드는 디지털 음의 씨앗입니다. 여기에 필터나 앰프, 모듈레이션 등을 적용해 최종 음색을 구성하게 됩니다. "
  },
  
  {
    "title": "AudioKit의 Balancer",
    "url": "/posts/Balancer/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-29 12:58:54 +0900",
    "content": "Balancer     코드 보기   이 코드는 AudioKit을 이용해 재생 중인 오디오(피아노 소스)와 지속적인 사인파(440Hz 등)를 비교하여 밸런스를 맞추는 오디오 신호처리 구조입니다. 아래에서 각 구성 요소와 오디오 흐름, 그리고 핵심 역할인 Balancer를 중심으로 설명드립니다.    클래스 목적  BalancerConductor는 AudioKit의 Balancer 노드를 활용해 두 오디오 신호(오디오 플레이어와 사인파 오실레이터)를 비교하여 레벨 균형을 맞춘 후 DryWetMixer로 두 소리를 혼합합니다. 사용자 조작으로는:     사인파 주파수(frequency)   오디오 재생 속도(rate)   Dry/Wet 믹스 비율(balance)을 조절할 수 있습니다.     신호 흐름  [Audio File] --(VariSpeed)--&gt; [Fader] --&gt;┐                                          ├──[Balancer]──┐ [Oscillator] ---------------------------&gt;┘             │                                                         ├─[DryWetMixer]─▶ Output [Fader] ------------------------------------------------┘     구성 요소 설명                 구성 요소       설명                       AudioPlayer       피아노 샘플을 불러와 루프 재생                 Oscillator       440Hz 사인파 생성. frequency로 조절 가능                 VariSpeed       피아노 재생 속도 조절. rate로 설정                 Fader       VariSpeed의 출력을 볼륨 조절용으로 래핑                 Balancer       오실레이터의 출력을 기준으로 fader 출력을 레벨 밸런싱                 DryWetMixer       원본(fader)과 보정된(balancer) 출력을 블렌딩                 Engine.output       최종 믹싱된 결과를 출력             @Published 변수                 변수       설명                       frequency       오실레이터의 주파수를 0.5초에 걸쳐 슬라이딩 조절 ramp(to: frequency, duration: 0.5)                 rate       VariSpeed를 통해 오디오 재생 속도 조절 (1.0이 원래 속도)                 balance       DryWetMixer의 블렌딩 비율 조절. 0 → 원본만 / 1 → 밸런서만             Balancer란?     Balancer(source, comparator)는 source의 레벨을 comparator에 맞춰 보정합니다.        이 경우:             source: 오실레이터 (톤 기준)       comparator: 피아노 파일 (재생된 사운드)           즉, 오실레이터 신호의 레벨이 피아노와 일치하도록 자동 조정됩니다.   이는 두 소스 간 일정한 레벨 밸런스를 유지할 때 유용합니다 (예: 모노소스 정렬, 분석 기준 맞춤 등).     🔍 실용적 의미     이 코드는 음향 비교, 레벨 정규화, 주파수 기반 음향 실험에 적합합니다.   특히 정현파(사인파) 기준으로 다른 소리의 볼륨을 맞추고 싶을 때, 이 구조가 적절합니다.   사용자는 실시간으로 주파수(frequency), 재생속도(rate), DryWet 비율(balance)를 변경해가며 음향 실험을 진행할 수 있습니다.     시각적 의미    기본 개념  Balancer는 다음 두 신호의 레벨 차이를 비교하고, source의 게인을 자동으로 조정합니다:     source: 조정 대상 (여기서는 osc = 사인파)   comparator: 기준 레벨 (여기서는 fader = 피아노 재생 소스)     시각적 예시  예: osc는 일정한 레벨의 사인파, fader는 볼륨이 점점 커졌다 작아지는 피아노 소리라고 할 때:  Time → --&gt;  comparator (피아노 볼륨):  ░░░░░▒▒▒▒▒▓▓▓▓▓██████▓▓▓▓▓▒▒▒▒▒░░░░░      &lt;-- 커졌다가 줄어듦  source (원래 오실레이터 출력):  ██████████████████████████████████      &lt;-- 항상 일정  Balancer(source, comparator) 출력:  ░░░░░▒▒▒▒▒▓▓▓▓▓██████▓▓▓▓▓▒▒▒▒▒░░░░░      &lt;-- comparator에 맞춰 조정됨   즉, osc의 신호는 실제로는 그대로지만, Balancer가 comparator의 레벨을 추적하여 볼륨을 자동으로 매칭합니다.    응용 시나리오     기준 신호(예: 백그라운드 노이즈나 기준톤)에 맞춰 다른 소리를 정렬   두 채널 간 음압 차이를 줄일 때   실시간 신호 비교 및 교정   “참조 기준” 대비 신호 변화를 분석하고자 할 때     이해를 돕기 위한 간단한 다이어그램:     +---------+          +-----------+         +----------+    |  Osc    |──┐       |  Fader    |──┐      |          |    | (source)|  ├─────▶ |(comparator)| ├────▶ | Balancer |──▶ Output    +---------+  │       +-----------+  │      +----------+                 │                      │                 └──────────────────────┘     용어 설명    🎧 1. 모노 소스 정렬 (Mono Source Matching / Alignment)          의미: 하나의 모노 오디오 신호를 다른 신호의 음량 또는 에너지 레벨에 맞춰 조정하는 것           예시: 믹스 중에 보컬 신호(모노)를 기준으로, 사인파(모노 톤)를 같은 음압으로 들리도록 맞추고 싶을 때 사용           실제 활용:             사운드 테스트용 기준 톤을 맞출 때       비교 대상이 되는 음성/악기 신호에 맞춰 임의 신호(예: 톤)를 정렬할 때       머신러닝/오디오 분석에서 “기준 볼륨”에 맞춰 데이터 정규화할 때             🔬 2. 분석 기준 맞춤 (Comparator Level Matching / Normalization)          의미: 특정 신호(분석 대상)의 레벨을 기준 신호(분석 기준)에 맞추는 작업           즉, “A 소리”를 “B 기준 레벨”에 맞춰서 자동 조정함           예시:             사운드 A를 기준 사운드 B의 에너지 레벨에 맞춰 비교 가능하도록 볼륨을 자동 보정       실시간으로 들어오는 마이크 입력을, 기준 신호에 따라 자동 볼륨 조정       예: 노이즈 프로파일에 맞춰 마이크 신호를 억제하거나 강조             🧠 쉽게 비유하자면:     기준이 되는 “모델 소리”가 있고, 그 기준에 따라 “학생 소리”가 그에 맞춰 음량을 조정당한다고 보면 됩니다. Balancer는 이 “모델 소리”를 comparator로 사용하고, source 신호를 모델에 맞게 맞춰주는 역할을 합니다.    "
  },
  
  {
    "title": "음향: 피드백(Feedback)",
    "url": "/posts/%ED%94%BC%EB%93%9C%EB%B0%B1(Feedback)/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-27 20:41:58 +0900",
    "content": "Feedback    음향에서 피드백(feedback)은 출력된 소리의 일부를 다시 입력으로 되돌려서 재사용하는 것을 말합니다. 주로 딜레이(Delay), 리버브(Reverb) 등의 이펙트에서 사용되며, 반복되거나 잔향이 남는 소리를 만드는 데 쓰입니다.    예시          딜레이 이펙트가 있는 오디오에서:             원래 소리: “Hello”       피드백이 없을 때 → “Hello (딱 한 번 반복됨)”       피드백이 있을 때 → “Hello… hello… hello…” 점점 작아지며 반복됨             기술적인 설명     피드백 값이 0이면 한 번만 반향이 생기고 바로 사라짐   피드백 값이 1에 가까울수록 소리가 더 오랫동안, 더 많이 반복됨        예:             feedback = 0.3: “Hello” → echo 1~2번 들리고 끝남       feedback = 0.9: “Hello” → 수초간 반복됨             음악적 활용     앰비언트/패드 사운드에서 공간감을 주기 위해 사용   디지털 딜레이에서 텍스처를 풍부하게 만듦   라이브 퍼포먼스에서는 효과적인 사운드 디자인 도구     즉, “되돌아오는 소리”의 세기가 피드백입니다. "
  },
  
  {
    "title": "AudioKit의 STKEnsemble",
    "url": "/posts/STKEnsemble/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-27 20:41:58 +0900",
    "content": "STK Ensemble   https://github.com/user-attachments/assets/fcab9142-670f-4026-a257-19ec9dce263e     코드 보기   이 코드는 AudioKit, STKAudioKit, 그리고 SwiftUI를 활용하여 세 개의 가상 악기(Flute, Clarinet, Tubular Bells)를 조합한 **음악 합주 시스템(Ensemble Generator)**입니다. 주기적으로 음을 무작위로 선택하여 세 악기가 연주되며, 각 악기의 볼륨을 조절하고, 스케일(Scale)과 전조(Transpose)를 실시간으로 반영할 수 있습니다.    주요 구성 요소  STK 악기     Flute, Clarinet, TubularBells: STKAudioKit의 물리 기반 모델링 악기   trigger(note:velocity:): 지정한 MIDI 음과 벨로시티로 소리를 내는 메서드   stop(): 악기의 소리를 끈다   Fader     각 악기에 대한 볼륨 조절   fluteFader, clarinetFader, bellsFader는 각각 대응하는 악기를 감싼 Fader   .gain을 통해 실시간 볼륨 조절 가능   Mixer 및 AudioEngine     Mixer(fluteFader, clarinetFader, bellsFader)로 세 악기 음원을 혼합   engine.output = mixer를 통해 최종 출력 연결   CallbackLoop     loop = CallbackLoop(frequency:)는 지정된 주기(Hz)마다 클로저를 실행   loop.start() / loop.stop()으로 실행 제어   현재 playRate가 1.67이면 약 0.6초 간격으로 실행됨     음원 생성 로직  let scale = [60, 62, 64, 66, 67, 69, 71]      MIDI 노트 값으로 C Lydian 스케일   let transposedScale = [... down + base + up ...]      전조(transpose)를 반영하고,   원래 스케일의 뒷부분은 한 옥타브 아래, 앞부분은 한 옥타브 위로 보강하여 음역 확대   if random(0.45) { flute.trigger(...) }      악기마다 확률을 다르게 하여 음을 낼지 결정   무작위로 노트를 선택하고 trigger() 호출   벨로시티는 30~100 사이 무작위 (randomVelocity())     상태 변수                 변수       설명                       playRate       loop 주파수 (Hz 단위, 루프 반복 속도)                 scale       현재 사용하는 스케일 (MIDI 음 배열)                 transpose       스케일 전조 값                 fluteGain       플루트 볼륨                 clarinetGain       클라리넷 볼륨                 bellsGain       벨 볼륨                 playingNotes       최근 연주된 노트 (악기별)             주요 함수  random(_ probability: Double) -&gt; Bool     0~1 사이 확률로 true 반환 → 연주 유무를 확률적으로 결정하는 데 사용   randomVelocity(...)     벨로시티(세기)를 랜덤으로 생성 → 자연스러운 음색 변화   stopAllInstruments()     세 악기를 모두 멈춤 (음이 겹치거나 끊기지 않게 관리)     활용 예시  이 클래스를 SwiftUI에서 사용하면,     스케일 선택기, 트랜스포즈 슬라이더, 볼륨 조절 노브 등 UI를 통해 실시간으로 음원을 조작할 수 있음   예:  Slider(value: $conductor.fluteGain, in: 0...1) Picker(\"Scale\", selection: $conductor.scale) { ... }     정리  STKEnsembleConductor는 오디오 합성을 위한 재생 루프, 확률 기반 음 선택, 실시간 볼륨 조절, 스케일 조작, STK 악기 연주를 모두 포함한 오디오 합주 클래스입니다. 음향적으로도, 무작위성과 악기 간의 주파수 분산이 혼합되어 자연스럽고 흥미로운 합주 효과를 만들어 냅니다. "
  },
  
  {
    "title": "AudioKit의 PluckedString",
    "url": "/posts/PluckedString/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-27 15:35:34 +0900",
    "content": "Plucked String     코드 보기   이 코드는 AudioKit과 SoundpipeAudioKit을 사용하여 플럭 현악기(plucked string) 소리를 자동 생성하고, 여기에 딜레이와 리버브 효과를 적용하는 구조입니다. 아래에 각 구성요소별로 자세히 설명드리겠습니다.    주요 구성 요소 설명  let pluckedString = PluckedString()     SoundpipeAudioKit의 PluckedString 노드입니다.   실제 기타, 하프 등 줄을 튕기는 소리의 물리적 모델링으로 소리를 생성합니다.   trigger()를 호출하면 소리가 발생합니다.   frequency: 재생될 음의 주파수   amplitude: 음의 크기   playRate = 3.0     초당 3번 루프가 반복됨을 의미합니다.   CallbackLoop(frequency:)에서 사용되어, 약 0.33초마다 트리거가 발생합니다.     init() – 노드 연결 및 효과 설정          DryWetMixer(pluckedString, pluckedString2)             두 플럭 사운드를 합칩니다.       dry/wet 구분 없이 두 음원을 섞는 믹서입니다.                Delay(mixer)             딜레이 효과 적용       delay.time = 1.5 / playRate → 약 0.5초 딜레이       delay.feedback = 0.9: 반복된 신호의 강도가 강함 (잔향 길어짐)                Reverb(delay)             전체 사운드에 리버브를 걸어 공간감을 추가       dryWetMix = 0.9: 대부분의 신호를 리버브 처리                engine.output = reverb             오디오 엔진 출력으로 설정             loop = CallbackLoop(frequency: playRate)     1초에 3번 반복하는 콜백 루프   콜백 안에서는 다음이 실행됩니다:   let scale = [60, 62, 64, 66, 67, 69, 71]      C 리디안 스케일입니다.        MIDI Note Number이며 각각 다음과 같습니다:             C4, D4, E4, F♯4, G4, A4, B4           let note1 = Int.random(...) let note2 = Int.random(...)      무작위로 두 음 선택 → 두 개의 pluckedString에 할당   if AUValue.random(...) &gt; 15 {   pluckedString.trigger()   pluckedString2.trigger() }      확률적으로 50%의 확률로 둘 다 트리거     @Published var isRunning     SwiftUI와 바인딩되어 있으며, 사용자가 ON/OFF 조작 시 loop.start() 또는 loop.stop()을 실행     요약                 구성 요소       설명                       PluckedString       줄을 튕기는 악기 사운드 생성                 CallbackLoop       일정 주기로 콜백 호출 (음 트리거용)                 Delay, Reverb       공간감 있는 음향 효과 적용                 isRunning       루프 시작/정지 상태           이 코드는 자동으로 리듬감 있게 연주되는 플럭 스트링을 만들며, 풍부한 잔향과 공간감을 주기 위해 딜레이와 리버브를 결합하여 앰비언트 음악이나 사운드 디자인에 적합한 사운드를 구성합니다. "
  },
  
  {
    "title": "AudioKit의 VocalTractOperation",
    "url": "/posts/VocalTractOperation/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-26 14:05:58 +0900",
    "content": "Vocal Tract Operation     코드 보기   이 코드는 AudioKit의 OperationGenerator를 사용해 인간의 음성을 물리적으로 모델링한 음향 합성기를 구성합니다. vocalTract는 성도(목구멍~입)의 구조를 모사한 음향 모델이며, 다양한 인자에 따라 음색이 변합니다. 아래는 주요 변수와 연산자의 역할을 중점적으로 설명합니다.    🔁 전체 구조  let generator = OperationGenerator { ... }      Swift 클로저 안에서 AudioKit Operation DSL을 사용해 신호 흐름을 정의합니다.   반환값은 Operation.vocalTract(...): 인간 음성을 시뮬레이션하는 메인 오퍼레이션     🔊 각 파라미터 설명  1. 성문 주파수 (Glottal Frequency)  let frequency = Operation.sineWave(frequency: 1)   .scale(minimum: 100, maximum: 300)      sineWave(frequency: 1) → 1Hz 속도로 천천히 진동   .scale(minimum: 100, maximum: 300) → 100~300Hz 범위로 변환   사람 목소리의 음 높이 범위에 해당하는 값   2. 지터 (Jitter)  let jitter = Operation.jitter(   amplitude: 300,   minimumFrequency: 1,   maximumFrequency: 3 ).scale()      지터: 음성의 미세한 주파수 흔들림 → 자연스러움/불안정성 부여   amplitude: 300 → 최대 300Hz 변화폭   .scale() → -1 ~ 1 사이의 랜덤값을 가진 사인파로 정규화됨   3. 혀 위치 (Tongue Position)  let position = Operation.sineWave(frequency: 0.1).scale()      아주 느리게(0.1Hz) 움직이는 사인파   .scale()은 기본적으로 -1 ~ 1로 스케일링됨   혀의 앞뒤 위치를 조정 → 모음 종류 변화   4. 혀 직경 (Tongue Diameter)  let diameter = Operation.sineWave(frequency: 0.2).scale()      혀의 높낮이 조절 → 음색 변화   5. 긴장도 (Tenseness)  let tenseness = Operation.sineWave(frequency: 0.3).scale()      성대의 긴장도 → 목소리의 강약, 날카로움, 부드러움에 영향   6. 비음도 (Nasality)  let nasality = Operation.sineWave(frequency: 0.35).scale()      비강(코)으로 얼마나 소리를 흘리는지를 나타냄 → 콧소리 비율 제어     🔚 최종 합성  return Operation.vocalTract(   frequency: frequency + jitter,   tonguePosition: position,   tongueDiameter: diameter,   tenseness: tenseness,   nasality: nasality )      vocalTract는 위 파라미터들을 이용해 목소리의 음높이, 모음형, 억양을 종합적으로 결정   frequency + jitter → 안정된 음 높이 + 지터를 통한 자연스러움 부여     💡 요약                 파라미터       역할       효과 예시                       frequency       기본 음 높이 (톤)       남성·여성 목소리 음역 차이 조정                 jitter       주파수 흔들림       더 자연스럽고 인간다운 발성 구현                 position       혀 위치 (전방/후방)       [i], [u], [a] 같은 모음 차이                 diameter       혀의 위아래 위치       열림 모음/닫힘 모음 구분 등                 tenseness       성대 긴장       부드럽거나 긴장된 발성                 nasality       비음도 (콧소리 성분)       영어 [n], [m], [ŋ] 같은 발음 구현           이 모델은 실제로도 Pink Trombone 알고리즘 기반이며, 사람의 음성 생성 메커니즘을 물리적으로 시뮬레이션합니다. "
  },
  
  {
    "title": "SwiftUI: @Binding을 지원하지 않는 뷰의 강제 리프레시",
    "url": "/posts/%EB%B7%B0%EA%B0%95%EC%A0%9C%EB%A6%AC%ED%94%84%EB%A0%88%EC%8B%9C/",
    "categories": "개발, Swift, SwiftUI",
    "tags": "SwiftUI",
    "date": "2025-05-14 18:04:00 +0900",
    "content": "SwiftUI에서 @Binding을 지원하지 않는 뷰의 강제 리프레시  parameters와 ParameterRow가 바인딩(Binding)을 사용하지 않고 있다면, SwiftUI는 내부 값이 바뀌더라도 해당 뷰(HStack)를 자동으로 업데이트하지 않습니다. 따라서 Randomize 버튼을 눌러도 뷰가 갱신되지 않는 문제가 발생합니다.  해결 방법: id 값을 바꿔서 뷰 강제 갱신  HStack 전체에 .id(...)를 주어 SwiftUI가 해당 뷰를 다른 것으로 인식하도록 하면 뷰 전체가 강제로 다시 그려집니다.  @State private var refreshID = UUID()   그 다음, HStack에 .id(refreshID)를 적용합니다:  HStack {   ForEach(conductor.voc.parameters.indices, id: \\.self) {     ParameterRow(param: conductor.voc.parameters[$0])   } } .id(refreshID) // &lt;- 이게 핵심   그리고 Randomize 버튼에서 refreshID를 갱신해주면 SwiftUI는 뷰 전체를 새로 렌더링합니다:  Button(\"Randomize\") {   conductor.voc.frequency = AUValue.random(in: 0...2000)   conductor.voc.tonguePosition = AUValue.random(in: 0...1)   conductor.voc.tongueDiameter = AUValue.random(in: 0...1)   conductor.voc.tenseness = AUValue.random(in: 0...1)   conductor.voc.nasality = AUValue.random(in: 0...1)   refreshID = UUID() // 강제 갱신 트리거 }     대안 (권장): Binding을 지원하도록 구조 수정     VocalTract.parameters가 @Published var 배열이면 @Binding으로 넘길 수 있습니다.   ParameterRow(param: Binding&lt;NodeParameter&gt;)처럼 정의하고, 수정 가능하게 만들 수도 있습니다.   하지만 외부 라이브러리이거나 구조를 못 바꾸는 경우에는 .id(UUID()) 방식이 가장 확실하고 간단한 방법입니다.    결론  parameters가 Binding을 지원하지 않더라도 HStack에 .id(refreshID)를 설정하고 UUID()를 변경하는 방식으로 뷰를 강제로 새로 그리게 만들 수 있습니다. "
  },
  
  {
    "title": "AudioKit의 VocalTract",
    "url": "/posts/VocalTract/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-14 18:01:32 +0900",
    "content": "VocalTract     코드 보기   용어설명  성도 (Vocal Tract, 聲道)     정의: 인간의 발성 기관 중, 성문부터 입술까지의 공기 통로를 의미합니다.   구성: 인두(pharynx), 구강(oral cavity), 비강(nasal cavity) 등 포함.   역할: 이 통로의 형태와 크기를 조절함으로써 소리의 공명을 변화시켜 다양한 말소리(모음, 자음)를 만들어냅니다. 예: 입을 크게 벌리거나 혀 위치를 바꾸면 음색이 달라지는 이유.   성문 펄스 파형 (Glottal Pulse Wave)     정의: 성대(glottis)가 주기적으로 열리고 닫히며 발생시키는 기초적인 소리 파형입니다.   역할: 이 파형이 바로 음성의 원천적인 진동이며, 이후 성도에서 공명을 거쳐 실제 말소리로 바뀝니다.   예시: 남자의 저음 목소리는 느린 주기로 성대가 열리고 닫히면서 낮은 주파수의 성문 펄스가 발생하는 것.   요약하면:     성도는 소리를 변조하는 필터 역할이고,   성문 펄스 파형은 소리의 원천 신호 (진동원) 역할입니다.     파라미터  VocalTract에서 조정하는 아래 5가지 파라미터는 사람의 음성 생성 과정을 물리적으로 모사한 것이며, 각각이 소리의 성질에 중요한 영향을 미칩니다. 아래는 각 파라미터가 조절될 때 어떤 음향적 특징이 변하는지에 대한 설명입니다.  1. frequency: 성문 주파수 (Glottal Frequency)     정의: 성대의 진동 속도, 즉 1초당 성대가 열리고 닫히는 횟수 (Hz).        영향:             이 값이 높을수록 음의 높이(피치)가 올라감.       낮을수록 음성이 굵고 낮은 톤.           예시: 어린이의 목소리는 높은 주파수, 남성의 저음 목소리는 낮은 주파수.   2. tonguePosition: 혀 위치     정의: 입안에서 혀의 앞뒤 위치를 0~1 범위로 나타냄.        영향:             혀가 앞쪽(0)일수록 앞 모음 계열 (예: [i], [e])       뒤쪽(1)으로 갈수록 후설음 계열 (예: [u], [o])           예시: ‘이’와 ‘우’의 차이처럼 혀 위치에 따라 공명 위치가 달라짐.   3. tongueDiameter: 혀 직경     정의: 혀가 차지하는 입안의 공간 크기를 나타냄.        영향:             작을수록 입안의 공간이 좁아져 높은 포먼트 주파수 → 밝고 가는 소리       클수록 공간이 넓어져 낮은 포먼트 주파수 → 둔탁하고 어두운 소리           예시: 입을 크게 벌리거나 혀를 들어올릴 때 음색이 달라짐.   4. tenseness: 음성 긴장도     정의: 성대 근육의 긴장 정도 (0 = 무성음, 1 = 긴장된 유성음)        영향:             값이 낮으면 속삭이는 소리, 숨소리 같이 부드럽고 기식이 섞임       값이 높으면 선명한 유성음, 일반적인 목소리           예시: whisper와 normal voice의 차이   5. nasality: 비음도 (콧소리 성분)     정의: 코를 통한 공기 통과 정도        영향:             값이 높을수록 콧소리가 섞인 듯한 음색       낮을수록 입 중심의 일반적인 소리           예시: “나”, “마”처럼 코를 울리는 소리에서 nasality 값이 높음   요약표                 파라미터       설명       영향을 주는 소리 특성                       frequency       성대 진동 속도       음의 높낮이 (피치)                 tonguePosition       혀의 앞뒤 위치       발음의 공명 위치 (모음의 구분)                 tongueDiameter       혀의 굵기/공간       음색의 밝기/어둠                 tenseness       성대의 긴장도       유성/무성 구분, 속삭임 ↔ 선명도                 nasality       비강의 개방 정도       비음의 정도 (콧소리 포함 여부)           이러한 파라미터들은 함께 작동하여 하나의 음성이 생성되며, 이를 조절하면 사람의 말소리나 다양한 음색을 실제처럼 시뮬레이션할 수 있게 됩니다. "
  },
  
  {
    "title": "AudioKit의 Sporth OperationGenerator",
    "url": "/posts/Sporth_OperationGenerator/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-12 15:19:44 +0900",
    "content": "Sporth  Sporth는 “Small Portable Real-Time Sound Language”의 줄임말로, 경량 텍스트 기반 오디오 DSP 언어입니다. C 기반 오디오 라이브러리인 Soundpipe의 일부로 개발되었으며, AudioKit에서도 내장되어 있습니다.    ✅ Sporth란?     목적: 실시간 오디오 신호 처리를 빠르고 간단하게 기술할 수 있도록 설계된 스택 기반 언어   형식: \"sine 440 0.5 mul\" 같은 단순한 텍스트 문자열   실행: 오디오 그래프(DSP 신호 흐름)를 실시간으로 생성하고 처리함   용도: 합성(synthesis), 필터링, 이펙트, 시퀀싱 등 오디오 관련 처리 전반     🔧 Sporth의 작동 방식 (스택 기반)  스택 머신처럼 동작합니다:  440 sine 0.5 mul   이 의미는:     440 → 스택에 440 푸시   sine → 스택에서 440을 꺼내 사인파 생성 → 결과 푸시   0.5 → 0.5 푸시   mul → 두 값을 꺼내 곱함 → 최종 오디오 신호 생성     🧪 예시  1. 사인파 생성  440 sine   → 440Hz 사인파 생성  2. 노이즈 생성 + 필터  pinknoise 8000 tone   → 핑크 노이즈를 8kHz로 로우패스 필터링  3. 오토펜닝 + 볼륨 조절  0.5 0.5 sine pan 0.2 mul   → 오토펜닝된 사운드를 볼륨 20%로 출력    🎯 Sporth의 장점                 항목       설명                       가볍고 빠름       DSP를 직접 제어하는 데 적합함                 실시간 수정       오디오 중단 없이 코드를 바꿀 수 있음                 AudioKit 통합       OperationGenerator에서 쉽게 사용 가능             🧩 AudioKit과의 관계  AudioKit에서는 OperationGenerator(sporth: \"코드\") 형태로 Sporth를 사용할 수 있습니다. 복잡한 UI 없이도 DSP 알고리즘을 간단히 실험하거나 프로토타이핑할 수 있는 좋은 도구입니다.    OperationGenerator  OperationGenerator는 AudioKit에서 제공하는 Node의 서브클래스로, Sporth 코드 기반의 사운드 생성기(Generator) 역할을 합니다. 즉, 사용자가 제공하는 Sporth DSL(도메인 특화 언어) 또는 AudioKit의 Operation API를 바탕으로 오디오 신호를 생성하는 가변형 DSP 노드입니다.    ✅ 주요 특징  1. Node 프로토콜 채택          OperationGenerator는 AudioKit의 Node를 상속받으며,             connections는 빈 배열 (연결된 노드 없음)       avAudioNode는 내부적으로 instantiate(instrument: \"cstg\")를 통해 생성된 AVAudioNode             2. 파라미터 14개 제공     최대 14개의 파라미터 (parameter1 ~ parameter14)를 지원   각 파라미터는 @Parameter 프로퍼티 래퍼를 사용하여 연결됨   실제 AUParameter로 연결되어 외부에서 실시간 조절 가능 (예: UI 슬라이더)     3. 여러 초기화 방식 지원  a. 단일 Operation 기반 초기화  OperationGenerator(operation: { ops in   ops[0].sineWave() })      입력된 ([Operation]) -&gt; ComputedParameter 클로저로부터 Sporth 코드를 생성   단일 채널인 경우 dup, 스테레오인 경우 swap 등을 Sporth 코드에 삽입하여 채널 정렬   b. 파라미터 없는 Operation도 가능  OperationGenerator(operation: {   Operation.sineWave() })   c. 스테레오 Operation 두 개 입력  OperationGenerator(channelCount: 2, operations: { ops in   [ops[0].sineWave(), ops[1].squareWave()] })     4. Sporth 문자열 직접 초기화 가능  OperationGenerator(sporth: \"pinknoise dup\")      Sporth 문자열을 직접 넣어 제너레이터를 구성할 수 있음   내부적으로 akOperationSetSporth()를 통해 DSP 설정     5. trigger()로 사운드 실행  generator.trigger()      au.trigger()를 호출하여 현재 파라미터로 한 번 실행   주로 “보이스”, “샘플”, “효과음” 등에 사용     🔧 활용 예시  let generator = OperationGenerator {   Operation.sineWave(frequency: Operation.parameter(1)) * 0.3 } generator.parameter1 = 440 generator.start() generator.trigger()     요약                 요소       설명                       Operation 기반       AudioKit Operation API 또는 Sporth로 구성                 파라미터       최대 14개 실시간 제어용                 출력       mono 또는 stereo                 주요 용도       사운드 합성, 효과 생성, 실험적 DSP 노드 구성            "
  },
  
  {
    "title": "SpriteKitAudio 예제",
    "url": "/posts/SpriteKitAudio/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-11 16:17:36 +0900",
    "content": "Sprite Kit Audio     코드 보기   이 코드는 SpriteKit과 AudioKit을 결합한 인터랙티브 사운드 예제입니다. 사용자가 화면을 터치하면 공이 생성되고, 플랫폼에 부딪히면 소리가 나는 구조입니다. 주요 구성요소별로 설명드리겠습니다.    ✅ 전반적 구조     SpriteKit을 이용해 물리 기반 애니메이션 처리   AudioKit을 이용해 플랫폼에 충돌 시 음향 재생   SwiftUI에서 SpriteView로 SpriteKit 장면을 보여줌     1. GameScene (SKScene + 물리 처리)  didMove(to:)     장면 초기 설정   physicsWorld.contactDelegate = self → 충돌 감지 델리게이트 설정   physicsBody = edgeLoopFrom: → 화면 가장자리를 물리적으로 막는 테두리 벽 생성   for i in 1...3 → 3개의 경사진 **플랫폼(plat)**을 생성하고 물리 바디 설정   각 플랫폼 설정     회전각 .pi / 8, -.pi / 8을 사용해 경사 조절   categoryBitMask = 2, contactTestBitMask = 2로 충돌 범주 설정   isDynamic = false로 고정된 물체 (움직이지 않음)     2. touchesBegan(_:with:)          사용자가 화면을 터치하면:             반지름 5짜리 원형 공(box) 생성       랜덤한 색상 적용       해당 위치에 공 생성 후 중력 적용 (affectedByGravity = true)       category와 contact 비트마스크도 2번으로 설정 → 플랫폼과 충돌 감지 가능             3. didBegin(_:)          공이 플랫폼에 닿을 때:             platform1 → MIDI note 60       platform2 → MIDI note 64       platform3 → MIDI note 67       → 각각 다른 음을 재생 (도, 미, 솔)                그 외 바닥에 닿으면 ball 노드를 제거        4. playSound(noteNumber:)     MIDISampler를 이용해 해당 note를 0.1초 동안 연주 후 중지     5. SpriteKitAudioConductor     MIDISampler를 AudioKit에서 초기화   .exs 사운드폰트 로딩 (e.g. sawPiano1.exs)   Reverb로 출력 노드 감쌈 → 약간의 공간감 제공     6. SpriteKitAudioView (SwiftUI)     GameScene을 SpriteView에 넣어 SwiftUI에서 표시   conductor를 장면에 주입   .onAppear / .onDisappear로 AudioKit 엔진 제어     🔊 동작 흐름 요약     화면 터치 → 공 생성 → 중력에 의해 하강   공이 플랫폼에 닿으면 → 해당 음 재생   바닥에 닿으면 → 공 제거     SpriteKit 부분 상세  SpriteKit 부분은 이 프로젝트에서 그래픽과 물리 시뮬레이션을 담당하며, 주로 GameScene 클래스 내에서 구성됩니다. 각 구성 요소를 더 구체적으로 설명드리겠습니다.    🧱 GameScene 구성 요소 상세  📍 didMove(to:)  SpriteKit 장면이 화면에 처음 표시될 때 호출되는 메서드입니다. 여기서 물리 월드 설정과 플랫폼 배치를 합니다.  주요 구성:    physicsWorld.contactDelegate = self   → 물리 충돌 이벤트를 감지하기 위해 SKPhysicsContactDelegate 프로토콜을 채택하고, 해당 delegate를 자기 자신으로 지정합니다.    physicsBody = SKPhysicsBody(edgeLoopFrom: frame)   → 화면 전체를 테두리로 감싸는 물리 벽 생성. 공이 밖으로 나가지 않도록 함.    for i in 1...3 {     ...     addChild(plat)   }   → 경사진 3개의 플랫폼을 반복문으로 생성. 각 플랫폼은 SKShapeNode로 구성되고, 물리 바디가 설정됩니다.  각 플랫폼의 설정:  plat.physicsBody = SKPhysicsBody(rectangleOf: CGSize(width: 80, height: 10)) plat.physicsBody?.isDynamic = false plat.physicsBody?.affectedByGravity = false plat.physicsBody?.categoryBitMask = 2 plat.physicsBody?.contactTestBitMask = 2      isDynamic = false → 위치가 고정되어 물리 반응은 있지만, 움직이지 않음   categoryBitMask &amp; contactTestBitMask → 충돌 감지 대상 설정 (같은 그룹끼리 감지 가능)     📍 touchesBegan(_:with:)  사용자가 화면을 터치하면 호출되는 메서드입니다.     원형 SKShapeNode(box)를 생성   터치 위치에 배치   중력 영향을 받도록 설정 (affectedByGravity = true)   충돌 감지를 위해 물리 바디 설정   예:  let box = SKShapeNode(circleOfRadius: 5) box.physicsBody = SKPhysicsBody(circleOfRadius: 5) box.physicsBody?.affectedByGravity = true box.physicsBody?.categoryBitMask = 2 box.physicsBody?.contactTestBitMask = 2   이 공은 중력으로 낙하하며, platform과 category 설정이 같기 때문에 충돌 시 didBegin(_:) 호출됨.    📍 didBegin(_ contact: SKPhysicsContact)  SpriteKit의 충돌 이벤트가 발생했을 때 자동 호출되는 메서드입니다.     contact.bodyA와 contact.bodyB를 통해 어떤 노드끼리 충돌했는지 확인   충돌한 platform이 어떤 것인지에 따라 다른 MIDI 노트(60, 64, 67)를 재생   또한, 특정 노드가 바닥이나 화면 하단에 닿은 경우 제거 처리도 여기에 포함될 수 있습니다.    🎯 SpriteKit에서 이 예제가 하는 일 요약                 구성 요소       역할                       SKScene       물리 공간 (게임 보드 같은 역할)                 SKPhysicsWorld       중력과 충돌 계산 수행                 SKShapeNode       공 및 플랫폼 노드의 시각적 표현                 SKPhysicsBody       실제 물리 연산을 담당하는 요소                 categoryBitMask, contactTestBitMask       충돌 범주 정의 및 감지 설정                 touchesBegan       공 생성 및 사용자 입력 반응                 didBegin       충돌 시 사운드 재생 로직 실행            "
  },
  
  {
    "title": "랜덤 노이즈(random noise)란?",
    "url": "/posts/%EB%9E%9C%EB%8D%A4%EB%85%B8%EC%9D%B4%EC%A6%88/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-09 15:56:00 +0900",
    "content": "랜덤 노이즈  화이트 노이즈, 핑크 노이즈, 브라우니안(Brownian) 노이즈는 모두 스펙트럼 에너지 분포가 다른 랜덤 신호입니다. 이들은 소리의 특성과 에너지 분포를 다르게 하며, 오디오 테스트, 사운드 디자인, 백색소음 치료, 모듈레이션 등에 사용됩니다.  아래에 각각의 정의, 청각적 특징, 생성 방식(알고리즘)을 정리해 드리겠습니다.    1. 화이트 노이즈 (White Noise)  정의     모든 주파수 대역에 같은 에너지를 가지는 노이즈   스펙트럼 밀도: 0dB/옥타브   주파수당 에너지 분포가 균등함     청각적 특징     샤아아아아 소리 (TV 안테나 잡음과 유사)   고역이 강조되어 다소 날카롭게 들림   생성 방식  // 샘플마다 랜덤한 값을 생성 sample = Float.random(in: -1.0...1.0)      단순히 uniform 랜덤 값으로 오디오 버퍼를 채우면 됩니다.     2. 핑크 노이즈 🌸 (Pink Noise)  정의     주파수가 2배가 될수록 에너지가 절반으로 줄어듦   스펙트럼 밀도: -3dB/옥타브   인간 청각 특성에 더 자연스럽게 들리는 노이즈     청각적 특징     부드러운 바람소리, 빗소리와 유사   고역이 덜 강조됨   생성 방식          필터 방식:             화이트 노이즈 → 1/f 필터(로우패스 계열 필터)를 통과시켜 감쇠       예: IIR 필터나 FIR 필터 사용                Voss-McCartney 알고리즘:             여러 개의 서로 다른 주기에서 값을 바꾸는 난수 발생기       각 샘플마다 일부만 갱신하여 저역 강조됨                오디오키트 예시:      let pinkNoise = PinkNoise(amplitude: 0.5)      내부적으로는 고역을 점차 감쇠시키는 방식 사용     3. 브라우니안 노이즈 (Brown Noise = Brownian, 적색 노이즈)  정의     주파수 증가에 따라 에너지가 급격히 감소   스펙트럼 밀도: -6dB/옥타브   Brownian motion(브라운 운동)의 누적 효과와 유사     청각적 특징     무거운 저음 중심, 폭풍 소리 느낌   고역 거의 없음   생성 방식  // 이전 샘플값에 현재 난수값을 누적 var lastSample = 0.0 let currentSample = lastSample + Double.random(in: -0.05...0.05) lastSample = currentSample      즉, 화이트 노이즈를 적분한 형태 → 신호가 누적되면서 저역 성분이 강해짐     요약 비교                 구분       에너지 분포       청각 특징       생성 방식 요약                       화이트 노이즈       평탄(0dB/옥타브)       샤아아, 고역 많음       random(-1...1)                 핑크 노이즈       -3dB/옥타브 감소       자연음에 가까움       필터 or Voss-McCartney                 브라운 노이즈       -6dB/옥타브 감소       무겁고 둔탁한 저음       누적형: y[n] = y[n-1] + rand            "
  },
  
  {
    "title": "음향: CutOff(컷오프) 주파수란?",
    "url": "/posts/CutOff%EC%A3%BC%ED%8C%8C%EC%88%98/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-08 14:01:23 +0900",
    "content": "CutOff Frequency  컷오프 주파수(Cutoff Frequency)란, 오디오 필터가 신호를 걸러내기 시작하는 주파수의 경계점을 의미합니다.   설명  로우패스 필터(Low-pass filter)의 경우:     컷오프 주파수 이하의 저주파 성분은 통과   컷오프 주파수 이상의 고주파 성분은 감쇠 또는 제거   즉, 컷오프 주파수가 5,000Hz면 → 5kHz보다 낮은 음은 통과시키고, 높은 음은 줄이거나 제거합니다.    컷오프 주파수의 수치 범위  음향에서 일반적인 컷오프 주파수 범위는 다음과 같습니다:     20Hz ~ 20,000Hz (20kHz) → 인간의 청각 범위 전체   예: 30...20000 Hz 라고 쓰면 → 30Hz 이하나 20kHz 이상은 거의 모두 걸러짐     왜 조정하나?  필터의 **톤(tone)**을 바꾸기 위해서입니다.     컷오프를 낮추면 → 소리가 더 어두워짐   컷오프를 높이면 → 고음이 살아남아 밝은 소리     예제: 정규화된 Frequency가 0.8 → 0.2로 줄어들면?  frequency.denormalized(to: 30...20000, taper: 3)에서 frequency가 0.8 → 0.2로 줄어들면, 다음과 같은 의미입니다:     0.8은 상대적으로 높은 컷오프 주파수 → 예: 약 12,000Hz 근처   0.2는 상대적으로 낮은 컷오프 주파수 → 예: 약 100~200Hz 근처   즉, denormalized는 0.0~1.0 사이의 정규화된 값을 로그 스케일로 30Hz~20000Hz 범위에 매핑해주는 함수입니다.    대략적인 값 예시 (taper: 3 기준, 로그 스케일):                 정규화 값       컷오프 주파수 (예상값)                       0.0       30 Hz                 0.2       약 120~200 Hz                 0.5       약 1,000 Hz                 0.8       약 12,000 Hz                 1.0       20,000 Hz           따라서 → 0.8 → 0.2로 바꾸면 → 컷오프 주파수는 약 12,000Hz → 200Hz로 급격히 낮아지며, → 소리가 어두워지고 고음이 잘려나가며 묵직해지는 느낌이 납니다.    denormalized(to:taper:)는 AudioKit에서 사용하는 보간 함수로, 0.0~1.0 값을 실사용 주파수로 비선형(log taper) 변환할 때 사용됩니다.    Log Taper란?  Log taper(로그 테이퍼)는 값의 변화가 로그함수 곡선처럼 느리게 시작하고 점점 빠르게 증가하는 방식의 곡선을 말합니다.    왜 log taper를 쓰나?  사람의 귀는 소리의 변화를 선형(linear)이 아닌 로그(logarithmic) 방식으로 인식하기 때문입니다.     예: 주파수 100Hz → 200Hz는 큰 차이로 느껴지고 10,000Hz → 10,100Hz는 거의 차이 없음     log taper vs. linear taper                 입력 값       Linear 결과       Log taper 결과 (예시)                       0.0       30 Hz       30 Hz                 0.25       5087 Hz       250 Hz                 0.5       10,015 Hz       1000 Hz                 0.75       14,972 Hz       5000 Hz                 1.0       20,000 Hz       20,000 Hz              Linear: 중간값도 10,000Hz 근처 (느끼기엔 대부분 고음만 변함)   Log: 중간값이 1,000Hz로, 사람 귀에 더 자연스럽고 유효한 조절 가능     실제 적용 예     EQ(이퀄라이저)의 컷오프 주파수 노브   디지털 신디사이저의 파라미터 슬라이더   UI 슬라이더 값 → 실 주파수/볼륨 매핑 등     Swift에서 taper가 적용된 예  value.denormalized(to: 30...20000, taper: 3)      taper: 1 → linear   taper: &gt;1 → log taper (ex: 3이면 더 강한 곡선)     아래는 정규화된 값(0.0\\~1.0)을 기준으로 한 log taper 와 linear taper의 대략적인 비교 그래프입니다. 가로축은 입력값 (x: 0.0 ~ 1.0), 세로축은 실제 주파수 출력 (y: 30Hz ~ 20000Hz)입니다.  y ↑ │ log taper       ◉ │               ◉ │             ◉ │           ◉ │         ◉ │       ◉ │    ◉ │ ◉ │ │ │ ◉  ◉  ◉  ◉  ◉  ◉  ◉  ◉  ◉◉◉◉  linear taper └──────────────────────────────→ x   0.0       0.5       1.0   해석:     Linear taper는 일정하게 증가 → 대부분 고음에서만 세밀 조절 가능   Log taper는 저역에서 더 섬세하게 조절 가능 → 사람이 듣기에 더 자연스럽고 유용   예를 들어 0.2, 0.4, 0.6, 0.8 정규화 값을 넣었을 때:                 입력값 (x)       Linear (Hz)       Log Taper (Hz, taper:3)                       0.2       ~4026       ~100                 0.4       ~8011       ~500                 0.6       ~11996       ~2000                 0.8       ~15981       ~7000             요약하자면, log taper는 사람의 감각에 맞춘 비선형 조절 방식이며, 보통 UI 슬라이더나 오디오 파라미터에서는 log taper를 사용합니다.   "
  },
  
  {
    "title": "AudioKit의 MIDIMonitor",
    "url": "/posts/MIDIMonitor/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-06 16:18:38 +0900",
    "content": "MIDI Monitor  이 MIDIMonitor.swift 파일은 SwiftUI 기반의 MIDI 모니터 도구입니다. MIDI 장치로부터 수신된 이벤트들을 실시간으로 감지하고 UI로 표시하는 역할을 합니다. AudioKit, SwiftUI, CoreMIDI를 활용하며, 사용자가 MIDI 장치의 입력을 확인하고 디버깅할 수 있도록 돕습니다.    🔧 주요 구조 설명  1. MIDIMonitorData 구조체  struct MIDIMonitorData {   var noteOn, velocity, noteOff, channel, afterTouch, afterTouchNoteNumber, programChange, pitchWheelValue, controllerNumber, controllerValue: Int }      각종 MIDI 이벤트 값을 저장하는 데이터 구조입니다.   @Published로 선언되어 View에서 실시간 바인딩됩니다.     2. MIDIMonitorConductor 클래스  ObservableObject를 채택한 ViewModel 역할의 클래스입니다.  주요 프로퍼티     midi: AudioKit의 MIDI 인터페이스 객체   data: MIDI 이벤트 데이터   isShowingMIDIReceived: MIDI 입력을 수신 중일 때 indicator 표시 여부   isToggleOn: CC 값이 127일 때 on 상태 (ex. 토글 스위치 UI용)   midiEventType: 이벤트 구분용 열거형   주요 메서드     start(): MIDI 입력 포트 열고 리스너 등록   stop(): MIDI 포트 닫기     3. MIDI 이벤트 수신 핸들러 (MIDIListener 구현)  AudioKit의 MIDIListener 프로토콜을 구현하여 다음 이벤트들을 처리합니다:  📌 receivedMIDINoteOn / receivedMIDINoteOff     노트 번호, 벨로시티, 채널을 추출해 데이터 갱신   벨로시티가 0일 경우 Note Off처럼 취급하여 isShowingMIDIReceived = false   📌 receivedMIDIController  if value == 127 {   self.isToggleOn = true } else {   self.isToggleOn = false }      CC 메시지 수신 시 컨트롤러 번호와 값 저장   value == 127이면 isToggleOn을 켜서 UI에 표시 (ex. 이펙터 스위치처럼 on/off 토글용)   📌 기타 이벤트     Program Change, Pitch Wheel, Aftertouch 등도 모두 개별적으로 처리하여 데이터를 반영     4. MIDIMonitorView 뷰     MIDIMonitorConductor를 @StateObject로 사용   수신된 MIDI 이벤트를 List의 Section으로 구분하여 표시   상단에 MIDI 입력 상태와 토글 여부를 나타내는 원형 인디케이터(Circle) 표시   인디케이터 예:  Circle()   .fill(conductor.isShowingMIDIReceived ? mainTintColor : mainTintColor.opacity(0.2))     🧪 특징 및 활용 예시     드럼패드를 누르면 Note On 이벤트 → 인디케이터 반응   노브를 돌리면 CC 메시지 → 컨트롤러 번호와 값 갱신   이펙터 on/off처럼 CC 값이 127 → isToggleOn = true로 빨간 불 표시    "
  },
  
  {
    "title": "MIDI에서 SFZ 포맷이란?",
    "url": "/posts/SFZ_%ED%8F%AC%EB%A7%B7/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-05 18:12:08 +0900",
    "content": "SFZ 포맷  SFZ 파일은 가상 악기용 샘플 기반 음원을 정의하는 텍스트 기반 포맷입니다. 주로 **샘플러(VSTi 등)**에서 다양한 악기 소리를 재생하기 위해 사용됩니다.    🔍 SFZ란?     확장자: .sfz   형식: 텍스트(ASCII)   역할: 오디오 샘플(WAV 등)의 배치/맵핑 정보를 정의   개발자: Cakewalk (RGC:audio)     🎵 어떻게 동작하나요?     여러 개의 WAV 샘플이 있음 (예: 각 음정별로 녹음된 피아노 소리).   sound.sfz 파일에서 각 샘플을 어떤 음정, 어떤 속도, 어떤 벨로시티에서 재생할지를 정의.   샘플러(예: Sforzando, ARIA, Kontakt 등)가 .sfz를 읽어 악기처럼 재생.   예시:  &lt;region&gt; sample=note_C4.wav lokey=60 hikey=60 pitch_keycenter=60   이건 C4(MIDI 60)에서만 note_C4.wav 샘플이 재생되도록 설정한 예입니다.    📌 SFZ의 특징                 장점       단점                       텍스트 기반으로 가볍고 수정 쉬움       GUI 지원 부족 (직접 코딩해야 함)                 무료 사용 및 확장 용이       복잡한 기능은 표현에 제약이 있음                 다양한 무료/유료 라이브러리 존재       제작 시 구조를 잘 이해해야 함             🔧 어디에 쓰이나요?     디지털 오디오 워크스테이션(DAW)에서 가상악기 로딩   무료/오픈소스 샘플러에서 악기 정의   게임 사운드 또는 개인 음악 제작용     🧾 코드 예제  &lt;group&gt;lokey=0 hikey=127 pitch_keycenter=57 pitch_keytrack=100 &lt;region&gt;lovel=000 hivel=127 amp_velcurve_127=1 loop_mode=loop_continuous loop_start=0 loop_end=220 sample=basicSamples/saw220.wav     🔍 해설  1. &lt;group&gt; 라인  &lt;group&gt;lokey=0 hikey=127 pitch_keycenter=57 pitch_keytrack=100      &lt;group&gt;: 이 뒤에 오는 &lt;region&gt;들에 공통으로 적용될 설정을 정의합니다.   lokey=0: 이 그룹의 음역 최소값은 MIDI 키 0 (C-1)입니다.   hikey=127: 최대값은 MIDI 키 127 (G9)입니다. → 모든 건반에 적용됨   pitch_keycenter=57: 이 샘플의 기준 피치는 MIDI 57 (A3)입니다.   pitch_keytrack=100: 피치 트래킹 100% → 누르는 키에 따라 정확히 해당 음정으로 피치 변경됩니다.     2. &lt;region&gt; 라인  &lt;region&gt;lovel=000 hivel=127 amp_velcurve_127=1 loop_mode=loop_continuous loop_start=0 loop_end=220 sample=basicSamples/saw220.wav      &lt;region&gt;: 실제 샘플을 매핑하는 설정입니다.   lovel=000 / hivel=127: 이 region은 모든 velocity(세기) 구간(0~127)에서 작동합니다.   amp_velcurve_127=1: velocity가 127일 때는 **볼륨 1.0(최대)**로 재생됩니다. → velocity에 따른 볼륨 곡선을 정의하는 파라미터.   loop_mode=loop_continuous: 샘플을 끝없이 루프 재생합니다.   loop_start=0 / loop_end=220: 0프레임부터 220프레임까지 루프합니다. → wav 파일 내에서 해당 영역이 반복됨.   sample=basicSamples/saw220.wav: 실제 재생할 샘플 파일 경로입니다. → basicSamples 폴더에 있는 saw220.wav라는 웨이브 파일 사용.     🎹 결과적으로 어떤 소리가 나나요?     사용자가 어떤 키를 누르든 (0~127), 어떤 velocity로 누르든 (0~127)   saw220.wav 파일이 기준음 A3로 pitch-shift 되어 재생됨.   소리는 0~220 프레임 사이를 무한 루프하며 지속됨.   velocity가 클수록 소리 크기는 커짐 (최대 127에서 1.0)    "
  },
  
  {
    "title": "음향: EQ(이퀄라이저), 중심 주파수 및 대역폭의 의미",
    "url": "/posts/EQ-%EC%A4%91%EC%8B%AC%EC%A3%BC%ED%8C%8C%EC%88%98_%EB%8C%80%EC%97%AD%ED%8F%AD/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-03 19:10:04 +0900",
    "content": "중심 주파수, 대역폭의 의미  **“중심 주파수(center frequency)가 32Hz이고, 대역폭(bandwidth)이 44.7Hz”**라는 말은, 해당 필터가 32Hz를 중심으로 한 주파수 범위에 영향을 미친다는 뜻입니다. 하지만 단순히 *“32 ± 44.7Hz”*와 같이 생각하면 안 됩니다. 이건 필터의 종류와 정의 방식에 따라 달라지기 때문입니다.    🎧 정확한 의미 요약  ✅ AudioKit의 EqualizerFilter에서의 bandwidth는 대역폭(Hz)     중심 주파수 f₀ = 32Hz   대역폭 BW = 44.7Hz   이때 필터는 다음과 같은 주파수 대역에 영향을 줍니다:  대략적으로: 32Hz ± (44.7Hz / 2) = 9.65Hz ~ 54.35Hz   즉, 필터는 약 9.65Hz부터 54.35Hz까지의 대역을 부스트 또는 컷하게 됩니다.     정확히 말하자면 “bandwidth”는 필터의 -3dB 지점 간 거리를 의미하며, 중심 주파수를 기준으로 그 좌우에 걸친 대역입니다.     📊 시각적 비유      Gain       ▲       │        _______       │       /       \\       │      /         \\       │_____/           \\______             |           |           (f₁)         (f₂)          9.65Hz       54.35Hz           ←  BW = 44.7Hz →             Center: 32Hz     ✅ 보충 설명: 필터 종류별 해석 차이                 필터 종류       bandwidth 의미                       Peak EQ (Bell)       중심 주파수 기준 -3dB 지점 간 거리                 Bandpass       허용 주파수 범위 (통과 영역)                 Shelf       대역폭이 아닌 경사만 조절              AudioKit의 EqualizerFilter는 내부적으로 bell 또는 band-pass EQ 구조를 사용하므로 위 해석이 적용됩니다.     -3dB의 의미  -3dB가 사용되는 이유는 오디오 및 신호 처리에서 전력(power) 기준으로 절반이 되는 지점을 의미하기 때문입니다. 이 기준은 필터의 대역폭, 컷오프, 주파수 응답의 경계 등을 정의할 때 표준적인 기준점으로 널리 사용됩니다.    📐 왜 -3dB인가?  오디오 필터의 주파수 응답에서:     0dB는 신호 감쇠(또는 증폭) 없이 통과됨을 의미합니다.   -3dB는 신호의 **전력(Power)**이 절반으로 줄어든 상태를 뜻합니다.   수식적으로 보면:  디시벨(dB)은 전력 비율일 때:  $$ \text{dB} = 10 "
  },
  
  {
    "title": "CoreAudio: kMIDINotPermitted 문제 (백그라운드 오디오 관련)",
    "url": "/posts/kMIDINotPermitted-%EB%AC%B8%EC%A0%9C/",
    "categories": "StudyLog, AudioKit",
    "tags": "AudioKit, 음향이론",
    "date": "2025-05-01 18:47:55 +0900",
    "content": "kMIDINotPermitted 문제  요약  iOS 앱에서 Background Modes의 “Audio, AirPlay, and Picture in Picture” 옵션을 활성화하지 않으면 MIDI 권한이 제한되어, MIDISampler나 MIDICallbackInstrument와 같은 실시간 MIDI 관련 기능이 제대로 작동하지 않습니다.    ✅ 오류 메시지 해석  CheckError.swift:CheckError(_:):176:kMIDINotPermitted: Have you enabled the audio background mode in your ios app?      kMIDINotPermitted는 MIDI 권한 부족을 나타냅니다.   “Have you enabled the audio background mode…?”라는 메시지 자체가 해결 방법을 알려주고 있습니다: → 즉, “Background Modes &gt; Audio”를 활성화해야 MIDI 기능이 허용됨을 뜻합니다.     🧠 왜 이런 문제가 발생하는가?  iOS는 에너지 절약과 보안을 이유로, 백그라운드 또는 오디오 처리 관련 기능을 명시적으로 허용하지 않으면 일부 오디오/MIDI 기능을 제한합니다.  특히 다음 경우:     AudioKit의 MIDISampler, AppleSequencer, MIDICallbackInstrument 등은 Core MIDI 또는 AVAudioEngine의 실시간 처리 권한이 필요합니다.   하지만 Info.plist 또는 Build 설정에 Background Modes &gt; Audio가 설정되지 않으면, iOS는 이 앱이 오디오 기반 앱이 아니라고 판단하고, → MIDI 처리를 차단합니다.     ✅ 해결 방법  1. Xcode에서 설정     프로젝트 설정 &gt; Signing &amp; Capabilities 탭   + Capability 버튼 클릭 → Background Modes 추가   Audio, AirPlay, and Picture in Picture 체크 ✅   2. Info.plist에 자동으로 추가되는 키  &lt;key&gt;UIBackgroundModes&lt;/key&gt; &lt;array&gt;   &lt;string&gt;audio&lt;/string&gt; &lt;/array&gt;     📌 정리                 항목       설명                       문제       MIDISampler 사용 시 MIDI 권한 부족으로 드럼 소리가 나지 않음                 원인       Background Modes &gt; Audio 옵션이 꺼져 있어서 iOS가 오디오/MIDI 권한 차단                 해결       Xcode &gt; Signing &amp; Capabilities &gt; Background Modes &gt; Audio 항목 체크                 효과       실시간 MIDI 입력, AudioKit 샘플러 및 시퀀서 정상 동작            "
  },
  
  {
    "title": "SwiftUI: @Observable 동작원리",
    "url": "/posts/Observable-%EB%8F%99%EC%9E%91%EC%9B%90%EB%A6%AC/",
    "categories": "개발, Swift, SwiftUI",
    "tags": "SwiftUI",
    "date": "2025-04-29 16:25:40 +0900",
    "content": "Observable 동작 원리 Q. viewModel에서 @State를 붙이지 않아도 코드가 동작하는 이유?  @Observable 클래스 AudioKit3DViewModel에서 참조하는   각 클래스 AudioEngine3DConductor, SceneCoordinator에  @Observable을 붙이지 않아도 동작하는 이유?   답변  1. viewModel에서 @State를 붙이지 않아도 코드가 동작하는 이유  @State는 SwiftUI 뷰 내부에서 값 타입(보통 구조체)을 직접 소유하고 변경할 때 사용됩니다. 그러나 viewModel은 클래스(AudioKit3DViewModel) 타입이며, 해당 클래스는 SwiftUI 뷰 외부에서 생성되어 참조됩니다. 이 경우 SwiftUI에서 값의 변경을 감지하려면 @ObservedObject, @StateObject, @EnvironmentObject 등의 속성이 필요합니다.  하지만 해당 코드에서는 @Observable 속성이 붙어 있는 것으로 보입니다.  Swift 5.9 이상 (또는 Xcode 15 이상)에서는 SwiftUI의 Observation 프레임워크를 기반으로 하는 @Observable 프로토콜이 도입되었으며, 이 프로토콜을 채택한 객체는 SwiftUI에서 자동으로 상태를 관찰할 수 있습니다. 이를 통해 @StateObject나 @ObservedObject 없이도 값의 변경이 뷰에 반영됩니다.  다시 말해, viewModel이 @Observable을 따르고 있고, SwiftUI가 이를 알 수 있기 때문에 별도의 속성 래퍼가 필요 없습니다. 단, 이 기능은 Swift의 Observation이 제대로 적용되는 환경(Xcode 15 이상, Swift 5.9 이상, iOS 17 이상 등)에서만 가능합니다.    2. AudioEngine3DConductor, SceneCoordinator에 @Observable을 붙이지 않아도 되는 이유  @Observable은 SwiftUI가 뷰에서 참조할 값이 변경될 때 UI를 자동 갱신하기 위해 사용하는 것입니다.  하지만 AudioEngine3DConductor와 SceneCoordinator는 뷰에서 직접 사용하는 것이 아니라, AudioKit3DViewModel 내부에서 사용됩니다. 그리고 이 클래스들은 값의 변경이 SwiftUI 뷰에 직접적인 영향을 주지 않기 때문에 @Observable을 붙일 필요가 없습니다.  즉, 다음과 같은 구조입니다:     AudioKit3DView는 viewModel: AudioKit3DViewModel을 소유함   AudioKit3DViewModel은 @Observable이라서 값의 변경을 SwiftUI가 감지할 수 있음   viewModel 내부에서 사용하는 conductor, coordinator는 뷰에 직접 바인딩된 값이 아님   따라서 그 내부 클래스에 @Observable을 붙일 필요가 없음   단, 만약 AudioEngine3DConductor나 SceneCoordinator의 속성을 뷰에서 직접 바인딩하거나 관찰해야 할 경우, 그때는 해당 클래스에도 @Observable이나 @Published 등을 적용해야 합니다.    요약                 항목       이유                       viewModel에 @State 없음       @Observable을 통해 SwiftUI가 자동 관찰하므로 별도 속성 래퍼 불필요                 conductor, coordinator에 @Observable 없음       뷰에서 직접 사용하지 않으며 뷰에 상태 영향을 주지 않기 때문          "
  }
  
]

